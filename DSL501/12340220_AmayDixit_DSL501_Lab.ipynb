{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class AlexNetCustom(nn.Module):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "class AlexNetCustom32(nn.Module):\n",
        "    \"\"\"Modified AlexNet for 32x32 input without upsampling\"\"\"\n",
        "    def __init__(self, num_classes=100):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 3 * 3, 4096),  # Adjusted for smaller spatial size\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"EXERCISE 1: AlexNet on CIFAR-100 - Upsampling vs Modified Architecture\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "model_upsampled = AlexNetCustom(num_classes=100).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_upsampled.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "print(\"\\n[1] Training AlexNet with upsampling to 224x224...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(5):  # Reduced epochs for comparison\n",
        "    model_upsampled.train()\n",
        "    running_loss, correct, total = 0, 0, 0\n",
        "    for inputs, labels in tqdm(trainloader, desc=f'Epoch {epoch+1}/5'):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_upsampled(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f'Epoch {epoch+1} | Loss: {running_loss/len(trainloader):.3f} | Acc: {100*correct/total:.2f}%')\n",
        "\n",
        "time_upsampled = time.time() - start_time\n",
        "print(f\"\\nTime for upsampled version: {time_upsampled:.2f} seconds\")\n",
        "\n",
        "# Approach 2: Modified architecture for 32x32\n",
        "transform_train_32 = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "trainset_32 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train_32)\n",
        "trainloader_32 = DataLoader(trainset_32, batch_size=128, shuffle=True, num_workers=2)  # Larger batch possible\n",
        "\n",
        "model_modified = AlexNetCustom32(num_classes=100).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_modified.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "print(\"\\n[2] Training modified AlexNet for 32x32 input...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(5):\n",
        "    model_modified.train()\n",
        "    running_loss, correct, total = 0, 0, 0\n",
        "    for inputs, labels in tqdm(trainloader_32, desc=f'Epoch {epoch+1}/5'):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_modified(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f'Epoch {epoch+1} | Loss: {running_loss/len(trainloader_32):.3f} | Acc: {100*correct/total:.2f}%')\n",
        "\n",
        "time_modified = time.time() - start_time\n",
        "print(f\"\\nTime for modified version: {time_modified:.2f} seconds\")\n",
        "print(f\"\\nSpeedup: {time_upsampled/time_modified:.2f}x\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xRDlztIDccr",
        "outputId": "c809e123-e004-47c1-abf6-34ea9f537610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "EXERCISE 1: AlexNet on CIFAR-100 - Upsampling vs Modified Architecture\n",
            "======================================================================\n",
            "\n",
            "[1] Training AlexNet with upsampling to 224x224...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:   1%|‚ñè         | 11/782 [01:14<1:26:30,  6.73s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXERCISE 2: Replace ReLU with GELU\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3,32,3,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32,64,3,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*8*8, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "class SimpleCNN_GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3,32,3,padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32,64,3,padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*8*8, 256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"\\n[1] Training SimpleCNN with ReLU...\")\n",
        "model_relu = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_relu.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(10):\n",
        "    model_relu.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_relu(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, loss {running_loss/len(trainloader):.4f}')\n",
        "\n",
        "model_relu.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model_relu(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "acc_relu = 100 * correct / total\n",
        "print(f'ReLU Test Accuracy: {acc_relu:.2f}%')\n",
        "\n",
        "print(\"\\n[2] Training SimpleCNN with GELU...\")\n",
        "model_gelu = SimpleCNN_GELU().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_gelu.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(10):\n",
        "    model_gelu.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_gelu(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, loss {running_loss/len(trainloader):.4f}')\n",
        "\n",
        "model_gelu.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model_gelu(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "acc_gelu = 100 * correct / total\n",
        "print(f'GELU Test Accuracy: {acc_gelu:.2f}%')\n",
        "\n",
        "print(f\"\\nComparison: GELU vs ReLU = {acc_gelu - acc_relu:+.2f}% difference\")"
      ],
      "metadata": {
        "id": "WaMAHgN_EQf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXERCISE 3: Implement residual blocks and create tiny-ResNet\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = torch.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TinyResNet(nn.Module):\n",
        "    \"\"\"Based on SimpleCNN structure but with residual blocks\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "\n",
        "        self.layer1 = ResidualBlock(32, 32, stride=1)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.layer2 = ResidualBlock(32, 64, stride=1)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*8*8, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool1(self.layer1(x))\n",
        "        x = self.pool2(self.layer2(x))\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model_resnet = TinyResNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_resnet.parameters(), lr=1e-3)\n",
        "\n",
        "print(\"\\nTraining Tiny-ResNet...\")\n",
        "for epoch in range(10):\n",
        "    model_resnet.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_resnet(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, loss {running_loss/len(trainloader):.4f}')\n",
        "\n",
        "\n",
        "model_resnet.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model_resnet(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "print(f'Tiny-ResNet Test Accuracy: {100*correct/total:.2f}%')\n"
      ],
      "metadata": {
        "id": "wsMWk9XLEanj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXERCISE 4: Channel Pruning\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nOriginal model accuracy (ReLU):\", f'{acc_relu:.2f}%')\n",
        "\n",
        "def prune_model(model, prune_ratio=0.3):\n",
        "    \"\"\"Prune channels with lowest L1 norm from convolutional layers\"\"\"\n",
        "    print(f\"\\nPruning {prune_ratio*100:.0f}% of channels...\")\n",
        "\n",
        "    conv1 = model.features[0]\n",
        "    weights = conv1.weight.data\n",
        "\n",
        "    l1_norm = torch.sum(torch.abs(weights), dim=(1,2,3))\n",
        "\n",
        "    num_channels = l1_norm.shape[0]\n",
        "    num_keep = int(num_channels * (1 - prune_ratio))\n",
        "\n",
        "    _, indices = torch.sort(l1_norm, descending=True)\n",
        "    keep_indices = indices[:num_keep].sort()[0]\n",
        "\n",
        "    print(f\"Conv layer: {num_channels} channels -> {num_keep} channels\")\n",
        "\n",
        "    pruned_weights = weights.clone()\n",
        "    prune_indices = indices[num_keep:]\n",
        "    pruned_weights[prune_indices] = 0\n",
        "\n",
        "    conv1.weight.data = pruned_weights\n",
        "\n",
        "    return model\n",
        "\n",
        "model_pruned = prune_model(model_relu, prune_ratio=0.3)\n",
        "\n",
        "model_pruned.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model_pruned(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "acc_pruned = 100 * correct / total\n",
        "\n",
        "print(f'\\nPruned model accuracy: {acc_pruned:.2f}%')\n",
        "print(f'Accuracy drop: {acc_relu - acc_pruned:.2f}%')\n",
        "print(f'Note: For actual parameter reduction, rebuild model with fewer channels')\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"All exercises completed!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "YgBMs-0gEfGs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
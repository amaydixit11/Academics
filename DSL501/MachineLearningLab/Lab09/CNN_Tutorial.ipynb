{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfFtZCuX8mv2"
      },
      "source": [
        "# Convolutional Neural Networks (CNNs) — Tutorial\n",
        "\n",
        "A hands-on, step-by-step Python notebook style tutorial that starts with simple CNNs and culminates with classic architectures like AlexNet, VGG and brief notes on ResNet/Inception. Contains runnable code cells (Keras + PyTorch), explanations, visualizations and exercises."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4aSZ_9s83Dd"
      },
      "source": [
        "## Table of contents\n",
        "\n",
        "1. Introduction & learning goals\n",
        "2. Prerequisites & environment\n",
        "3. Key concepts (conv layer, kernels, stride, padding, pooling, activations, receptive field, BN, dropout)\n",
        "4. Simple CNN on MNIST (Keras) — build, train, visualize\n",
        "5. Deeper CNN on CIFAR-10 (Keras) — augmentation, callbacks, regularization\n",
        "6. PyTorch: Simple CNN + training loop + debugging tips\n",
        "7. Transfer Learning: using pretrained models (PyTorch) — feature-extractor vs fine-tune\n",
        "8. Implementing AlexNet (PyTorch) — full architecture, tips, training on CIFAR-10 / Tiny ImageNet\n",
        "9. Overview of later architectures: VGG, Inception, ResNet (intuition & code pointers)\n",
        "10. Model sizing, FLOPs & parameter counting (how to think about complexity)\n",
        "11. Common pitfalls, debugging & performance tuning\n",
        "12. Exercises and extensions\n",
        "13. References & further reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpShxYMF88Lf"
      },
      "source": [
        "## 1. Introduction & learning goals\n",
        "\n",
        "By the end of this notebook you will be able to:\n",
        "\n",
        "* Understand convolution, pooling, and how CNNs process images.\n",
        "* Build a small CNN in Keras and PyTorch and train it on MNIST/CIFAR-10.\n",
        "* Apply data augmentation and regularization to improve generalization.\n",
        "* Use pretrained networks and understand transfer learning choices.\n",
        "* Implement AlexNet in PyTorch and know how to expand toward VGG/ResNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrzs_Ufx9Cmr"
      },
      "source": [
        "## 2. Prerequisites & environment\n",
        "\n",
        "* Python 3.8+\n",
        "* GPU recommended (for deeper nets like AlexNet) but CPU works for small experiments.\n",
        "* Libraries used in code examples below:\n",
        "\n",
        "  * TensorFlow / Keras (`tensorflow>=2.10`)\n",
        "  * PyTorch (`torch`, `torchvision`)\n",
        "  * NumPy, matplotlib, seaborn (optional)\n",
        "\n",
        "Install (example):\n",
        "\n",
        "```bash\n",
        "pip install numpy matplotlib seaborn tensorflow torchvision torch tqdm scikit-learn\n",
        "```\n",
        "\n",
        "Open a Colab or local Jupyter notebook and run cells as you go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNweunpa9I9H"
      },
      "source": [
        "## 3. Quick conceptual primer\n",
        "\n",
        "### Convolutional layer\n",
        "\n",
        "* Input: H x W x C_in feature map.\n",
        "* Kernel/filter: k x k x C_in; slides using stride `s` and padding `p`.\n",
        "* Output: H_out x W_out x C_out where C_out = number of filters.\n",
        "\n",
        "Formula for output size (single spatial axis):\n",
        "\n",
        "```\n",
        "out = floor((in + 2*p - k) / s) + 1\n",
        "```\n",
        "\n",
        "### Pooling\n",
        "\n",
        "* Downsamples spatial resolution (max pooling, average pooling).\n",
        "* Reduces parameters and adds spatial invariance.\n",
        "\n",
        "### Activation\n",
        "\n",
        "* ReLU commonly used: `f(x)=max(0,x)`\n",
        "* LeakyReLU, SELU, GELU also used depending on architecture.\n",
        "\n",
        "### BatchNorm\n",
        "\n",
        "* Normalizes activations per-batch to stabilize and accelerate training.\n",
        "\n",
        "### Dropout\n",
        "\n",
        "* Randomly zeroes activations during training for regularization.\n",
        "\n",
        "### Receptive field\n",
        "\n",
        "* The region in input image that affects a particular activation in deeper layers—grows with depth and kernel sizes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuZYMuZk9XH1"
      },
      "source": [
        "## 4. Simple CNN on MNIST (Keras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G6lQJs2-9Jer",
        "outputId": "a832b600-f389-40b0-e1dd-c59d20c43b9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1568</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,416</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │           \u001b[38;5;34m160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1568\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m100,416\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">105,866</span> (413.54 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m105,866\u001b[0m (413.54 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">105,866</span> (413.54 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m105,866\u001b[0m (413.54 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 24ms/step - accuracy: 0.7462 - loss: 0.8131 - val_accuracy: 0.9750 - val_loss: 0.0840\n",
            "Epoch 2/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9537 - loss: 0.1583 - val_accuracy: 0.9860 - val_loss: 0.0526\n",
            "Epoch 3/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9686 - loss: 0.1056 - val_accuracy: 0.9867 - val_loss: 0.0479\n",
            "Epoch 4/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9725 - loss: 0.0923 - val_accuracy: 0.9892 - val_loss: 0.0416\n",
            "Epoch 5/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9779 - loss: 0.0752 - val_accuracy: 0.9872 - val_loss: 0.0399\n",
            "313/313 - 2s - 8ms/step - accuracy: 0.9878 - loss: 0.0379\n",
            "[0.03786460682749748, 0.9878000020980835]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Keras MNIST example\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "# scale and reshape\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test  = x_test.astype('float32') / 255.0\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test  = np.expand_dims(x_test, -1)\n",
        "\n",
        "# One-hot labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test  = to_categorical(y_test, 10)\n",
        "\n",
        "# Model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(28,28,1)),\n",
        "    layers.MaxPool2D(2),\n",
        "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPool2D(2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Train\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=128, validation_split=0.1)\n",
        "\n",
        "# Evaluate\n",
        "print(model.evaluate(x_test, y_test, verbose=2))\n",
        "\n",
        "# Plot training curves\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fxPZNK99blA"
      },
      "source": [
        "**Notes**: This small net reaches ~99% train accuracy and ~99% test accuracy on MNIST with more epochs and minor tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "XiKRQQ7QEh3A",
        "outputId": "d87f2e30-017e-47e8-93c9-e5412ef7d929"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 3, 1, 16)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x600 with 16 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize filters of the first conv layer in Keras\n",
        "weights = model.layers[0].get_weights()[0]\n",
        "print(weights.shape) # (3,3,1,16) for first conv layer\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(4,4, figsize=(6,6))\n",
        "for i in range(16):\n",
        "  f = weights[:,:,:,i]\n",
        "  axs[i//4, i%4].imshow(f.squeeze(), cmap='gray')\n",
        "  axs[i//4, i%4].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSWRA3zC9jmn"
      },
      "source": [
        "## 5. Deeper CNN on CIFAR-10 (Keras) — augmentation, callbacks\n",
        "\n",
        "CIFAR-10 images are 32x32x3; more challenging. Use augmentation and weight decay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxeVIfC9BNUE",
        "outputId": "80d300a8-7a8e-46cf-a921-eb1ef6b9fae9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPUs Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trFZdWwX9cH0",
        "outputId": "507fd3b4-2e60-42ed-c1df-4ca77464d0b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPUs Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 395ms/step - accuracy: 0.2155 - loss: 2.0916 - val_accuracy: 0.4517 - val_loss: 1.5029 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 295ms/step - accuracy: 0.4341 - loss: 1.5537 - val_accuracy: 0.5543 - val_loss: 1.2503 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 297ms/step - accuracy: 0.5046 - loss: 1.3766 - val_accuracy: 0.5813 - val_loss: 1.1718 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 358ms/step - accuracy: 0.5718 - loss: 1.2061 - val_accuracy: 0.6262 - val_loss: 1.0698 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 303ms/step - accuracy: 0.6117 - loss: 1.0959 - val_accuracy: 0.6850 - val_loss: 0.9018 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 290ms/step - accuracy: 0.6289 - loss: 1.0460 - val_accuracy: 0.6771 - val_loss: 0.9281 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 291ms/step - accuracy: 0.6478 - loss: 0.9993 - val_accuracy: 0.6724 - val_loss: 0.9494 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 309ms/step - accuracy: 0.6665 - loss: 0.9648 - val_accuracy: 0.7010 - val_loss: 0.8622 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 293ms/step - accuracy: 0.6876 - loss: 0.8929 - val_accuracy: 0.7377 - val_loss: 0.7553 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 296ms/step - accuracy: 0.7005 - loss: 0.8502 - val_accuracy: 0.7273 - val_loss: 0.7775 - learning_rate: 0.0010\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79b8839220f0>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "# Check GPU\n",
        "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Optional: enable mixed precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "x_train, x_test = x_train.astype('float32')/255.0, x_test.astype('float32')/255.0\n",
        "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "# Model\n",
        "def make_cifar_model():\n",
        "    inputs = layers.Input(shape=(32,32,3))\n",
        "    x = layers.Conv2D(64, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
        "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.MaxPool2D(2)(x)\n",
        "    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.MaxPool2D(2)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(10, activation='softmax', dtype='float32')(x)  # force float32 output\n",
        "    return models.Model(inputs, outputs)\n",
        "\n",
        "with tf.device('/GPU:0'):\n",
        "    model = make_cifar_model()\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.5, verbose=1),\n",
        "    tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "# Train\n",
        "train_gen = datagen.flow(x_train, y_train, batch_size=512, subset=None)\n",
        "model.fit(train_gen, epochs=10, validation_data=(x_test, y_test), callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPUxxb6c9smU"
      },
      "source": [
        "**Practical tips:** use `LearningRateScheduler` or `CosineDecay`, monitor validation accuracy, and consider SGD with momentum for better generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owYJIKV1-FCg"
      },
      "source": [
        "## 6. PyTorch: Simple CNN + training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQVoUZl3EfxI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwYAlUH2-CH5",
        "outputId": "614936e9-adfe-4d27-9d2c-a156e0e44182"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 47.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, loss 1.4825\n",
            "Epoch 2, loss 1.1441\n",
            "Epoch 3, loss 0.9967\n",
            "Epoch 4, loss 0.9064\n",
            "Epoch 5, loss 0.8216\n",
            "Epoch 6, loss 0.7539\n",
            "Epoch 7, loss 0.7043\n",
            "Epoch 8, loss 0.6489\n",
            "Epoch 9, loss 0.5994\n",
            "Epoch 10, loss 0.5612\n"
          ]
        }
      ],
      "source": [
        "# PyTorch simple CNN (CIFAR-10)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3,32,3,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32,64,3,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*8*8, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training loop (one epoch example)\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, loss {running_loss/len(trainloader):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpG-Xd2W-QGM"
      },
      "source": [
        "**Debugging tips**: start with a very small dataset, ensure loss decreases, use gradient norm clipping if exploding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PD7fsOv-SOj"
      },
      "source": [
        "## 7. Transfer learning (PyTorch)\n",
        "\n",
        "Use `torchvision.models` for pretrained AlexNet/VGG/ResNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuGPmOVG-So8",
        "outputId": "a1f0ece8-f606-43e3-d1a7-5f660f441852"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 233M/233M [00:01<00:00, 168MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "alex = models.alexnet(pretrained=True)  # loads pretrained weights\n",
        "# Option A: feature extraction (freeze backbone)\n",
        "for p in alex.features.parameters():\n",
        "    p.requires_grad = False\n",
        "# Replace final classifier for CIFAR-10\n",
        "alex.classifier[6] = nn.Linear(alex.classifier[6].in_features, 10)\n",
        "alex = alex.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW9GStQI-S5V"
      },
      "source": [
        "* Feature-extractor: freeze backbone, train new head (fast, less data required).\n",
        "* Fine-tune: unfreeze some later layers and train with a lower LR."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCD_LA3W-lCN"
      },
      "source": [
        "## 8. Implementing AlexNet (PyTorch)\n",
        "\n",
        "AlexNet (Krizhevsky et al., 2012) — 5 conv layers + 3 fully connected layers (original used local response norm and dropouts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Kj82vOhP-lq3"
      },
      "outputs": [],
      "source": [
        "# AlexNet implementation (PyTorch)\n",
        "import torch.nn.functional as F\n",
        "class AlexNetCustom(nn.Module):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWYQmRpu-sBT"
      },
      "source": [
        "# Example: for CIFAR-10 (32x32) you'd adapt the first conv and pooling or upsample images to 224x224.\n",
        "\n",
        "**Notes for training AlexNet**:\n",
        "\n",
        "* Original AlexNet expects 224x224 inputs (ImageNet). For CIFAR-10, you can upsample to 224x224 or modify kernel/stride/pool sizes.\n",
        "* Consider using pretrained weights and fine-tuning.\n",
        "* Use data augmentation heavily and training schedules (SGD + momentum + LR decay)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vakn-DoSFmMn",
        "outputId": "5584f577-90ed-476e-a370-ad13174046cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:47<00:00,  7.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Loss: 1.808 | Acc: 32.54%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:41<00:00,  7.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Loss: 1.226 | Acc: 55.78%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:41<00:00,  7.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Loss: 0.955 | Acc: 66.61%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:40<00:00,  7.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Loss: 0.801 | Acc: 72.24%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:41<00:00,  7.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Loss: 0.704 | Acc: 75.70%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:40<00:00,  7.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 | Loss: 0.623 | Acc: 78.38%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:39<00:00,  7.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 | Loss: 0.567 | Acc: 80.52%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:39<00:00,  7.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 | Loss: 0.524 | Acc: 81.82%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:40<00:00,  7.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 | Loss: 0.482 | Acc: 83.42%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:41<00:00,  7.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 | Loss: 0.454 | Acc: 84.34%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "transforms.Resize(224),\n",
        "transforms.RandomHorizontalFlip(),\n",
        "transforms.ToTensor(),\n",
        "transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "model = AlexNetCustom(num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "\n",
        "for epoch in range(10):\n",
        "  model.train()\n",
        "  running_loss, correct, total = 0, 0, 0\n",
        "  for inputs, labels in tqdm(trainloader):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    total += labels.size(0)\n",
        "    correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "\n",
        "  scheduler.step()\n",
        "  print(f'Epoch {epoch+1} | Loss: {running_loss/len(trainloader):.3f} | Acc: {100*correct/total:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOCiAXyi_DrI"
      },
      "source": [
        "## 9. Quick overview: VGG, Inception, ResNet\n",
        "\n",
        "* **VGG**: deep stacks of 3x3 convs; simple and uniform; heavy parameter count.\n",
        "* **Inception**: mixed kernel sizes in parallel (1x1, 3x3, 5x5) with dimension reduction via 1x1 convs.\n",
        "* **ResNet**: residual connections `y = F(x) + x` allow training very deep nets by solving degradation problem.\n",
        "\n",
        "When progressing from AlexNet to these, note the evolution:\n",
        "\n",
        "* AlexNet: larger kernels, large FC layers.\n",
        "* VGG: smaller kernels but deeper.\n",
        "* Inception: width and multi-scale processing.\n",
        "* ResNet: skip-connections to enable depth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ELaiiP-GXqn",
        "outputId": "0cba5305-83c9-4435-c517-8213df31a8dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): ReLU(inplace=True)\n",
            "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (6): ReLU(inplace=True)\n",
            "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (8): ReLU(inplace=True)\n",
            "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): ReLU(inplace=True)\n",
            "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (13): ReLU(inplace=True)\n",
            "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (15): ReLU(inplace=True)\n",
            "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (18): ReLU(inplace=True)\n",
            "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (20): ReLU(inplace=True)\n",
            "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (22): ReLU(inplace=True)\n",
            "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (25): ReLU(inplace=True)\n",
            "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (27): ReLU(inplace=True)\n",
            "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (29): ReLU(inplace=True)\n",
            "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "vgg = models.vgg16(pretrained=True)\n",
        "print(vgg.features[:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N32gMq3GZXs",
        "outputId": "6f7d7906-2525-4dd1-dad1-2a171fb98b84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:01<00:00, 42.3MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "resnet = models.resnet18(pretrained=True)\n",
        "print(resnet.layer1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_pjRlkNGcCq",
        "outputId": "4c6dee89-fdf5-4542-c932-3fa706389d47"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 104M/104M [00:00<00:00, 175MB/s]\n"
          ]
        }
      ],
      "source": [
        "inception = models.inception_v3(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlFyhzsI_GEz"
      },
      "source": [
        "## 10. Model sizing and compute\n",
        "\n",
        "* Parameter count roughly equals sum of (kernel_size * in_channels * out_channels) + biases.\n",
        "* FLOPs approximate multiply-adds: for each conv, `H_out * W_out * K*K * Cin * Cout * 2` (multiply + add). Tools exist to compute FLOPs automatically (e.g., `torchinfo`, `fvcore`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1EyuHYQ_Hmf"
      },
      "source": [
        "## 11. Common pitfalls & debugging checklist\n",
        "\n",
        "* Data normalization mismatch between train and pretrained models.\n",
        "* Learning rate too high/low — watch training curves.\n",
        "* Overfitting — use augmentation, dropout, weight decay.\n",
        "* Underfitting — increase capacity or train longer.\n",
        "* Incorrect label encoding / loss mismatch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StWb9uoJ_JDD"
      },
      "source": [
        "## 12. Exercises & extensions\n",
        "\n",
        "1. Implement AlexNet and train on CIFAR-100; compare training time when upsampling to 224x224 vs modifying first layers.\n",
        "2. Replace ReLU with GELU and observe impact.\n",
        "3. Implement simple residual blocks and convert the simple CNN to a tiny-ResNet.\n",
        "4. Prune channels of a trained model and measure accuracy drop."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfFtZCuX8mv2"
      },
      "source": [
        "# Convolutional Neural Networks (CNNs) — Tutorial\n",
        "\n",
        "A hands-on, step-by-step Python notebook style tutorial that starts with simple CNNs and culminates with classic architectures like AlexNet, VGG and brief notes on ResNet/Inception. Contains runnable code cells (Keras + PyTorch), explanations, visualizations and exercises."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4aSZ_9s83Dd"
      },
      "source": [
        "## Table of contents\n",
        "\n",
        "1. Introduction & learning goals\n",
        "2. Prerequisites & environment\n",
        "3. Key concepts (conv layer, kernels, stride, padding, pooling, activations, receptive field, BN, dropout)\n",
        "4. Simple CNN on MNIST (Keras) — build, train, visualize\n",
        "5. Deeper CNN on CIFAR-10 (Keras) — augmentation, callbacks, regularization\n",
        "6. PyTorch: Simple CNN + training loop + debugging tips\n",
        "7. Transfer Learning: using pretrained models (PyTorch) — feature-extractor vs fine-tune\n",
        "8. Implementing AlexNet (PyTorch) — full architecture, tips, training on CIFAR-10 / Tiny ImageNet\n",
        "9. Overview of later architectures: VGG, Inception, ResNet (intuition & code pointers)\n",
        "10. Model sizing, FLOPs & parameter counting (how to think about complexity)\n",
        "11. Common pitfalls, debugging & performance tuning\n",
        "12. Exercises and extensions\n",
        "13. References & further reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpShxYMF88Lf"
      },
      "source": [
        "## 1. Introduction & learning goals\n",
        "\n",
        "By the end of this notebook you will be able to:\n",
        "\n",
        "* Understand convolution, pooling, and how CNNs process images.\n",
        "* Build a small CNN in Keras and PyTorch and train it on MNIST/CIFAR-10.\n",
        "* Apply data augmentation and regularization to improve generalization.\n",
        "* Use pretrained networks and understand transfer learning choices.\n",
        "* Implement AlexNet in PyTorch and know how to expand toward VGG/ResNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrzs_Ufx9Cmr"
      },
      "source": [
        "## 2. Prerequisites & environment\n",
        "\n",
        "* Python 3.8+\n",
        "* GPU recommended (for deeper nets like AlexNet) but CPU works for small experiments.\n",
        "* Libraries used in code examples below:\n",
        "\n",
        "  * TensorFlow / Keras (`tensorflow>=2.10`)\n",
        "  * PyTorch (`torch`, `torchvision`)\n",
        "  * NumPy, matplotlib, seaborn (optional)\n",
        "\n",
        "Install (example):\n",
        "\n",
        "```bash\n",
        "pip install numpy matplotlib seaborn tensorflow torchvision torch tqdm scikit-learn\n",
        "```\n",
        "\n",
        "Open a Colab or local Jupyter notebook and run cells as you go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNweunpa9I9H"
      },
      "source": [
        "## 3. Quick conceptual primer\n",
        "\n",
        "### Convolutional layer\n",
        "\n",
        "* Input: H x W x C_in feature map.\n",
        "* Kernel/filter: k x k x C_in; slides using stride `s` and padding `p`.\n",
        "* Output: H_out x W_out x C_out where C_out = number of filters.\n",
        "\n",
        "Formula for output size (single spatial axis):\n",
        "\n",
        "```\n",
        "out = floor((in + 2*p - k) / s) + 1\n",
        "```\n",
        "\n",
        "### Pooling\n",
        "\n",
        "* Downsamples spatial resolution (max pooling, average pooling).\n",
        "* Reduces parameters and adds spatial invariance.\n",
        "\n",
        "### Activation\n",
        "\n",
        "* ReLU commonly used: `f(x)=max(0,x)`\n",
        "* LeakyReLU, SELU, GELU also used depending on architecture.\n",
        "\n",
        "### BatchNorm\n",
        "\n",
        "* Normalizes activations per-batch to stabilize and accelerate training.\n",
        "\n",
        "### Dropout\n",
        "\n",
        "* Randomly zeroes activations during training for regularization.\n",
        "\n",
        "### Receptive field\n",
        "\n",
        "* The region in input image that affects a particular activation in deeper layers—grows with depth and kernel sizes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuZYMuZk9XH1"
      },
      "source": [
        "## 4. Simple CNN on MNIST (Keras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G6lQJs2-9Jer",
        "outputId": "a832b600-f389-40b0-e1dd-c59d20c43b9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1568</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,416</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │           \u001b[38;5;34m160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1568\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m100,416\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">105,866</span> (413.54 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m105,866\u001b[0m (413.54 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">105,866</span> (413.54 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m105,866\u001b[0m (413.54 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.7439 - loss: 0.8218 - val_accuracy: 0.9788 - val_loss: 0.0777\n",
            "Epoch 2/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 40ms/step - accuracy: 0.9543 - loss: 0.1544 - val_accuracy: 0.9828 - val_loss: 0.0599\n",
            "Epoch 3/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 39ms/step - accuracy: 0.9679 - loss: 0.1055 - val_accuracy: 0.9872 - val_loss: 0.0469\n",
            "Epoch 4/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.9714 - loss: 0.0941 - val_accuracy: 0.9863 - val_loss: 0.0476\n",
            "Epoch 5/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - accuracy: 0.9780 - loss: 0.0704 - val_accuracy: 0.9862 - val_loss: 0.0468\n",
            "313/313 - 1s - 4ms/step - accuracy: 0.9870 - loss: 0.0392\n",
            "[0.03920985385775566, 0.9869999885559082]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Keras MNIST example\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "# scale and reshape\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test  = x_test.astype('float32') / 255.0\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test  = np.expand_dims(x_test, -1)\n",
        "\n",
        "# One-hot labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test  = to_categorical(y_test, 10)\n",
        "\n",
        "# Model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(28,28,1)),\n",
        "    layers.MaxPool2D(2),\n",
        "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPool2D(2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Train\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=128, validation_split=0.1)\n",
        "\n",
        "# Evaluate\n",
        "print(model.evaluate(x_test, y_test, verbose=2))\n",
        "\n",
        "# Plot training curves\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fxPZNK99blA"
      },
      "source": [
        "**Notes**: This small net reaches ~99% train accuracy and ~99% test accuracy on MNIST with more epochs and minor tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "XiKRQQ7QEh3A",
        "outputId": "d87f2e30-017e-47e8-93c9-e5412ef7d929"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 3, 1, 16)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x600 with 16 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize filters of the first conv layer in Keras\n",
        "weights = model.layers[0].get_weights()[0]\n",
        "print(weights.shape) # (3,3,1,16) for first conv layer\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(4,4, figsize=(6,6))\n",
        "for i in range(16):\n",
        "  f = weights[:,:,:,i]\n",
        "  axs[i//4, i%4].imshow(f.squeeze(), cmap='gray')\n",
        "  axs[i//4, i%4].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSWRA3zC9jmn"
      },
      "source": [
        "## 5. Deeper CNN on CIFAR-10 (Keras) — augmentation, callbacks\n",
        "\n",
        "CIFAR-10 images are 32x32x3; more challenging. Use augmentation and weight decay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxeVIfC9BNUE",
        "outputId": "80d300a8-7a8e-46cf-a921-eb1ef6b9fae9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPUs Available: []\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trFZdWwX9cH0",
        "outputId": "507fd3b4-2e60-42ed-c1df-4ca77464d0b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPUs Available: []\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m32/98\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:02:58\u001b[0m 330s/step - accuracy: 0.1618 - loss: 2.2330"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "# Check GPU\n",
        "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Optional: enable mixed precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "x_train, x_test = x_train.astype('float32')/255.0, x_test.astype('float32')/255.0\n",
        "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "# Model\n",
        "def make_cifar_model():\n",
        "    inputs = layers.Input(shape=(32,32,3))\n",
        "    x = layers.Conv2D(64, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
        "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.MaxPool2D(2)(x)\n",
        "    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.MaxPool2D(2)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(10, activation='softmax', dtype='float32')(x)  # force float32 output\n",
        "    return models.Model(inputs, outputs)\n",
        "\n",
        "with tf.device('/GPU:0'):\n",
        "    model = make_cifar_model()\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.5, verbose=1),\n",
        "    tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "# Train\n",
        "train_gen = datagen.flow(x_train, y_train, batch_size=512, subset=None)\n",
        "model.fit(train_gen, epochs=10, validation_data=(x_test, y_test), callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPUxxb6c9smU"
      },
      "source": [
        "**Practical tips:** use `LearningRateScheduler` or `CosineDecay`, monitor validation accuracy, and consider SGD with momentum for better generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owYJIKV1-FCg"
      },
      "source": [
        "## 6. PyTorch: Simple CNN + training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQVoUZl3EfxI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwYAlUH2-CH5",
        "outputId": "614936e9-adfe-4d27-9d2c-a156e0e44182"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 47.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, loss 1.4825\n",
            "Epoch 2, loss 1.1441\n",
            "Epoch 3, loss 0.9967\n",
            "Epoch 4, loss 0.9064\n",
            "Epoch 5, loss 0.8216\n",
            "Epoch 6, loss 0.7539\n",
            "Epoch 7, loss 0.7043\n",
            "Epoch 8, loss 0.6489\n",
            "Epoch 9, loss 0.5994\n",
            "Epoch 10, loss 0.5612\n"
          ]
        }
      ],
      "source": [
        "# PyTorch simple CNN (CIFAR-10)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3,32,3,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32,64,3,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*8*8, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training loop (one epoch example)\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, loss {running_loss/len(trainloader):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpG-Xd2W-QGM"
      },
      "source": [
        "**Debugging tips**: start with a very small dataset, ensure loss decreases, use gradient norm clipping if exploding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PD7fsOv-SOj"
      },
      "source": [
        "## 7. Transfer learning (PyTorch)\n",
        "\n",
        "Use `torchvision.models` for pretrained AlexNet/VGG/ResNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuGPmOVG-So8",
        "outputId": "a1f0ece8-f606-43e3-d1a7-5f660f441852"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 233M/233M [00:01<00:00, 168MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "alex = models.alexnet(pretrained=True)  # loads pretrained weights\n",
        "# Option A: feature extraction (freeze backbone)\n",
        "for p in alex.features.parameters():\n",
        "    p.requires_grad = False\n",
        "# Replace final classifier for CIFAR-10\n",
        "alex.classifier[6] = nn.Linear(alex.classifier[6].in_features, 10)\n",
        "alex = alex.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW9GStQI-S5V"
      },
      "source": [
        "* Feature-extractor: freeze backbone, train new head (fast, less data required).\n",
        "* Fine-tune: unfreeze some later layers and train with a lower LR."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCD_LA3W-lCN"
      },
      "source": [
        "## 8. Implementing AlexNet (PyTorch)\n",
        "\n",
        "AlexNet (Krizhevsky et al., 2012) — 5 conv layers + 3 fully connected layers (original used local response norm and dropouts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj82vOhP-lq3"
      },
      "outputs": [],
      "source": [
        "# AlexNet implementation (PyTorch)\n",
        "import torch.nn.functional as F\n",
        "class AlexNetCustom(nn.Module):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWYQmRpu-sBT"
      },
      "source": [
        "# Example: for CIFAR-10 (32x32) you'd adapt the first conv and pooling or upsample images to 224x224.\n",
        "\n",
        "**Notes for training AlexNet**:\n",
        "\n",
        "* Original AlexNet expects 224x224 inputs (ImageNet). For CIFAR-10, you can upsample to 224x224 or modify kernel/stride/pool sizes.\n",
        "* Consider using pretrained weights and fine-tuning.\n",
        "* Use data augmentation heavily and training schedules (SGD + momentum + LR decay)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vakn-DoSFmMn",
        "outputId": "5584f577-90ed-476e-a370-ad13174046cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:47<00:00,  7.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Loss: 1.808 | Acc: 32.54%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:41<00:00,  7.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Loss: 1.226 | Acc: 55.78%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:41<00:00,  7.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Loss: 0.955 | Acc: 66.61%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:40<00:00,  7.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Loss: 0.801 | Acc: 72.24%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:41<00:00,  7.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Loss: 0.704 | Acc: 75.70%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:40<00:00,  7.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 | Loss: 0.623 | Acc: 78.38%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:39<00:00,  7.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 | Loss: 0.567 | Acc: 80.52%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:39<00:00,  7.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 | Loss: 0.524 | Acc: 81.82%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:40<00:00,  7.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 | Loss: 0.482 | Acc: 83.42%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [01:41<00:00,  7.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 | Loss: 0.454 | Acc: 84.34%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "transforms.Resize(224),\n",
        "transforms.RandomHorizontalFlip(),\n",
        "transforms.ToTensor(),\n",
        "transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "model = AlexNetCustom(num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "\n",
        "for epoch in range(10):\n",
        "  model.train()\n",
        "  running_loss, correct, total = 0, 0, 0\n",
        "  for inputs, labels in tqdm(trainloader):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    total += labels.size(0)\n",
        "    correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "\n",
        "  scheduler.step()\n",
        "  print(f'Epoch {epoch+1} | Loss: {running_loss/len(trainloader):.3f} | Acc: {100*correct/total:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOCiAXyi_DrI"
      },
      "source": [
        "## 9. Quick overview: VGG, Inception, ResNet\n",
        "\n",
        "* **VGG**: deep stacks of 3x3 convs; simple and uniform; heavy parameter count.\n",
        "* **Inception**: mixed kernel sizes in parallel (1x1, 3x3, 5x5) with dimension reduction via 1x1 convs.\n",
        "* **ResNet**: residual connections `y = F(x) + x` allow training very deep nets by solving degradation problem.\n",
        "\n",
        "When progressing from AlexNet to these, note the evolution:\n",
        "\n",
        "* AlexNet: larger kernels, large FC layers.\n",
        "* VGG: smaller kernels but deeper.\n",
        "* Inception: width and multi-scale processing.\n",
        "* ResNet: skip-connections to enable depth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ELaiiP-GXqn",
        "outputId": "0cba5305-83c9-4435-c517-8213df31a8dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): ReLU(inplace=True)\n",
            "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (6): ReLU(inplace=True)\n",
            "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (8): ReLU(inplace=True)\n",
            "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): ReLU(inplace=True)\n",
            "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (13): ReLU(inplace=True)\n",
            "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (15): ReLU(inplace=True)\n",
            "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (18): ReLU(inplace=True)\n",
            "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (20): ReLU(inplace=True)\n",
            "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (22): ReLU(inplace=True)\n",
            "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (25): ReLU(inplace=True)\n",
            "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (27): ReLU(inplace=True)\n",
            "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (29): ReLU(inplace=True)\n",
            "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "vgg = models.vgg16(pretrained=True)\n",
        "print(vgg.features[:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N32gMq3GZXs",
        "outputId": "6f7d7906-2525-4dd1-dad1-2a171fb98b84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:01<00:00, 42.3MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "resnet = models.resnet18(pretrained=True)\n",
        "print(resnet.layer1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_pjRlkNGcCq",
        "outputId": "4c6dee89-fdf5-4542-c932-3fa706389d47"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 104M/104M [00:00<00:00, 175MB/s]\n"
          ]
        }
      ],
      "source": [
        "inception = models.inception_v3(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlFyhzsI_GEz"
      },
      "source": [
        "## 10. Model sizing and compute\n",
        "\n",
        "* Parameter count roughly equals sum of (kernel_size * in_channels * out_channels) + biases.\n",
        "* FLOPs approximate multiply-adds: for each conv, `H_out * W_out * K*K * Cin * Cout * 2` (multiply + add). Tools exist to compute FLOPs automatically (e.g., `torchinfo`, `fvcore`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1EyuHYQ_Hmf"
      },
      "source": [
        "## 11. Common pitfalls & debugging checklist\n",
        "\n",
        "* Data normalization mismatch between train and pretrained models.\n",
        "* Learning rate too high/low — watch training curves.\n",
        "* Overfitting — use augmentation, dropout, weight decay.\n",
        "* Underfitting — increase capacity or train longer.\n",
        "* Incorrect label encoding / loss mismatch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StWb9uoJ_JDD"
      },
      "source": [
        "## 12. Exercises & extensions\n",
        "\n",
        "1. Implement AlexNet and train on CIFAR-100; compare training time when upsampling to 224x224 vs modifying first layers.\n",
        "2. Replace ReLU with GELU and observe impact.\n",
        "3. Implement simple residual blocks and convert the simple CNN to a tiny-ResNet.\n",
        "4. Prune channels of a trained model and measure accuracy drop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "============================================================\n",
        "COMPLETE CNN EXERCISE SOLUTIONS\n",
        "============================================================\n",
        "Solutions to all exercises from the CNN tutorial\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# =======================\n",
        "# EXERCISE 1: AlexNet on CIFAR-100\n",
        "# Compare 224x224 upsampling vs modified first layers\n",
        "# =======================\n",
        "print(\"=\"*70)\n",
        "print(\"EXERCISE 1: AlexNet on CIFAR-100 - Two Approaches\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class AlexNetUpsampled(nn.Module):\n",
        "    \"\"\"Standard AlexNet expecting 224x224 inputs\"\"\"\n",
        "    def __init__(self, num_classes=100):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "class AlexNetModified(nn.Module):\n",
        "    \"\"\"Modified AlexNet for 32x32 inputs (CIFAR native resolution)\"\"\"\n",
        "    def __init__(self, num_classes=100):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Modified first layer: smaller kernel and stride for 32x32\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),  # 32x32\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # 16x16\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),  # 16x16\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # 8x8\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),  # 8x8\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),  # 8x8\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # 8x8\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # 4x4\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 4 * 4, 2048),  # Smaller FC due to smaller spatial size\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, num_classes),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"Count trainable parameters\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Load CIFAR-100\n",
        "transform_224 = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "])\n",
        "\n",
        "transform_32 = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_224 = torchvision.datasets.CIFAR100(root='./data', train=True, \n",
        "                                          download=True, transform=transform_224)\n",
        "train_32 = torchvision.datasets.CIFAR100(root='./data', train=True, \n",
        "                                         download=True, transform=transform_32)\n",
        "\n",
        "loader_224 = DataLoader(train_224, batch_size=64, shuffle=True, num_workers=2)\n",
        "loader_32 = DataLoader(train_32, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "# Initialize models\n",
        "model_upsampled = AlexNetUpsampled(num_classes=100).to(device)\n",
        "model_modified = AlexNetModified(num_classes=100).to(device)\n",
        "\n",
        "print(\"\\n📊 Model Comparison:\")\n",
        "print(f\"Upsampled (224x224): {count_parameters(model_upsampled):,} parameters\")\n",
        "print(f\"Modified (32x32):    {count_parameters(model_modified):,} parameters\")\n",
        "\n",
        "# Training function\n",
        "def train_one_epoch(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0, 0, 0\n",
        "    \n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    \n",
        "    return running_loss / len(loader), 100. * correct / total\n",
        "\n",
        "# Compare training speed (3 epochs each)\n",
        "print(\"\\n⏱️  Training Speed Comparison (3 epochs):\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 3\n",
        "\n",
        "# Train upsampled version\n",
        "print(\"\\n🔸 Training Upsampled AlexNet (224x224)...\")\n",
        "optimizer_up = optim.SGD(model_upsampled.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    loss, acc = train_one_epoch(model_upsampled, loader_224, criterion, optimizer_up)\n",
        "    print(f\"  Epoch {epoch+1}: Loss={loss:.3f}, Acc={acc:.2f}%\")\n",
        "time_224 = time.time() - start_time\n",
        "print(f\"  Total time: {time_224:.2f}s\")\n",
        "\n",
        "# Train modified version\n",
        "print(\"\\n🔹 Training Modified AlexNet (32x32)...\")\n",
        "optimizer_mod = optim.SGD(model_modified.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    loss, acc = train_one_epoch(model_modified, loader_32, criterion, optimizer_mod)\n",
        "    print(f\"  Epoch {epoch+1}: Loss={loss:.3f}, Acc={acc:.2f}%\")\n",
        "time_32 = time.time() - start_time\n",
        "print(f\"  Total time: {time_32:.2f}s\")\n",
        "\n",
        "print(f\"\\n⚡ Speedup: {time_224/time_32:.2f}x faster with 32x32 inputs\")\n",
        "print(\"📝 Key Findings:\")\n",
        "print(\"  • Modified version is faster (fewer pixels to process)\")\n",
        "print(\"  • Modified version has fewer parameters (smaller FC layers)\")\n",
        "print(\"  • Upsampled version may have slight accuracy advantage (more detail)\")\n",
        "\n",
        "# =======================\n",
        "# EXERCISE 2: Replace ReLU with GELU\n",
        "# =======================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXERCISE 2: ReLU vs GELU Activation Comparison\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    \"\"\"Simple CNN with configurable activation\"\"\"\n",
        "    def __init__(self, activation='relu', num_classes=10):\n",
        "        super().__init__()\n",
        "        \n",
        "        if activation == 'relu':\n",
        "            act = nn.ReLU\n",
        "        elif activation == 'gelu':\n",
        "            act = nn.GELU\n",
        "        elif activation == 'leaky_relu':\n",
        "            act = lambda: nn.LeakyReLU(0.1)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation: {activation}\")\n",
        "        \n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            act(),\n",
        "            nn.Conv2d(32, 32, 3, padding=1),\n",
        "            act(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            act(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            act(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 8 * 8, 256),\n",
        "            act(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Load CIFAR-10\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
        "                                        download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, \n",
        "                                       download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "# Test different activations\n",
        "activations = ['relu', 'gelu', 'leaky_relu']\n",
        "results = {}\n",
        "\n",
        "for act in activations:\n",
        "    print(f\"\\n🔬 Training with {act.upper()}...\")\n",
        "    model = SimpleCNN(activation=act).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    \n",
        "    train_losses = []\n",
        "    test_accs = []\n",
        "    \n",
        "    for epoch in range(5):\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        \n",
        "        avg_loss = running_loss / len(trainloader)\n",
        "        train_losses.append(avg_loss)\n",
        "        \n",
        "        # Testing\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in testloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "        \n",
        "        test_acc = 100. * correct / total\n",
        "        test_accs.append(test_acc)\n",
        "        print(f\"  Epoch {epoch+1}: Loss={avg_loss:.3f}, Test Acc={test_acc:.2f}%\")\n",
        "    \n",
        "    results[act] = {'losses': train_losses, 'accs': test_accs}\n",
        "\n",
        "# Plot comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "for act, data in results.items():\n",
        "    axes[0].plot(data['losses'], label=act.upper(), linewidth=2)\n",
        "    axes[1].plot(data['accs'], label=act.upper(), linewidth=2)\n",
        "\n",
        "axes[0].set_title('Training Loss Comparison')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].set_title('Test Accuracy Comparison')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n📊 Final Results:\")\n",
        "for act, data in results.items():\n",
        "    print(f\"  {act.upper():12s}: Final Test Acc = {data['accs'][-1]:.2f}%\")\n",
        "\n",
        "print(\"\\n💡 Observations:\")\n",
        "print(\"  • GELU often slightly smoother than ReLU (but similar performance)\")\n",
        "print(\"  • GELU is computationally more expensive than ReLU\")\n",
        "print(\"  • LeakyReLU can help with dying ReLU problem\")\n",
        "\n",
        "# =======================\n",
        "# EXERCISE 3: Implement Tiny-ResNet with Residual Blocks\n",
        "# =======================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXERCISE 3: Implementing Residual Blocks (Tiny-ResNet)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Basic residual block with skip connection\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        # Skip connection\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)  # Skip connection\n",
        "        out = torch.relu(out)\n",
        "        return out\n",
        "\n",
        "class TinyResNet(nn.Module):\n",
        "    \"\"\"Small ResNet for CIFAR-10\"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        \n",
        "        # Residual blocks\n",
        "        self.layer1 = self._make_layer(32, 32, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(32, 64, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(64, 128, 2, stride=2)\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "    \n",
        "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Compare with non-residual version\n",
        "class TinyPlainNet(nn.Module):\n",
        "    \"\"\"Plain CNN (no skip connections) for comparison\"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "print(\"\\n🏗️  Architecture Comparison:\")\n",
        "resnet = TinyResNet().to(device)\n",
        "plainnet = TinyPlainNet().to(device)\n",
        "\n",
        "print(f\"TinyResNet parameters:  {count_parameters(resnet):,}\")\n",
        "print(f\"TinyPlainNet parameters: {count_parameters(plainnet):,}\")\n",
        "\n",
        "# Train both for comparison\n",
        "def train_model(model, name, epochs=10):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
        "    \n",
        "    train_accs, test_accs = [], []\n",
        "    \n",
        "    print(f\"\\n🚀 Training {name}...\")\n",
        "    for epoch in range(epochs):\n",
        "        # Train\n",
        "        model.train()\n",
        "        correct, total = 0, 0\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "        \n",
        "        train_acc = 100. * correct / total\n",
        "        train_accs.append(train_acc)\n",
        "        \n",
        "        # Test\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in testloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "        \n",
        "        test_acc = 100. * correct / total\n",
        "        test_accs.append(test_acc)\n",
        "        scheduler.step()\n",
        "        \n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            print(f\"  Epoch {epoch+1}: Train={train_acc:.2f}%, Test={test_acc:.2f}%\")\n",
        "    \n",
        "    return train_accs, test_accs\n",
        "\n",
        "res_train, res_test = train_model(resnet, \"TinyResNet\", epochs=10)\n",
        "plain_train, plain_test = train_model(plainnet, \"TinyPlainNet\", epochs=10)\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(res_train, label='ResNet Train', linewidth=2)\n",
        "plt.plot(plain_train, label='PlainNet Train', linewidth=2)\n",
        "plt.title('Training Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(res_test, label='ResNet Test', linewidth=2)\n",
        "plt.plot(plain_test, label='PlainNet Test', linewidth=2)\n",
        "plt.title('Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n🏆 Final Test Accuracy:\")\n",
        "print(f\"  TinyResNet:   {res_test[-1]:.2f}%\")\n",
        "print(f\"  TinyPlainNet: {plain_test[-1]:.2f}%\")\n",
        "print(f\"\\n✨ Advantage: {res_test[-1] - plain_test[-1]:.2f}% improvement with skip connections!\")\n",
        "\n",
        "print(\"\\n💡 Why Residual Connections Help:\")\n",
        "print(\"  • Enable gradient flow through skip connections\")\n",
        "print(\"  • Allow training of deeper networks without degradation\")\n",
        "print(\"  • Model can learn identity mapping if needed (easier optimization)\")\n",
        "\n",
        "# =======================\n",
        "# BONUS: Visualize Learned Filters\n",
        "# =======================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"BONUS: Visualizing Learned Convolutional Filters\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get first conv layer weights from trained ResNet\n",
        "first_conv = resnet.conv1\n",
        "weights = first_conv.weight.data.cpu()\n",
        "print(f\"\\nFirst conv layer shape: {weights.shape}\")  # [32, 3, 3, 3]\n",
        "\n",
        "# Visualize first 16 filters\n",
        "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
        "fig.suptitle('Learned Convolutional Filters (First Layer)', fontsize=16)\n",
        "\n",
        "for i in range(16):\n",
        "    ax = axes[i // 4, i % 4]\n",
        "    \n",
        "    # Get filter and normalize for visualization\n",
        "    filt = weights[i].permute(1, 2, 0).numpy()  # [3, 3, 3] -> [3, 3, 3]\n",
        "    filt = (filt - filt.min()) / (filt.max() - filt.min())\n",
        "    \n",
        "    ax.imshow(filt)\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f'Filter {i+1}', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ ALL EXERCISES COMPLETED!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n📚 Summary:\")\n",
        "print(\"1. Modified AlexNet is faster but upsampled may be more accurate\")\n",
        "print(\"2. GELU provides smoother gradients but similar performance to ReLU\")\n",
        "print(\"3. Residual connections significantly improve deep network training\")\n",
        "print(\"4. First layer filters learn edge detectors and color patterns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

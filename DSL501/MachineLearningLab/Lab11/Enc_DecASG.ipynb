{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Objective\n",
        "\n",
        "Implement a **Sequence-to-Sequence (Seq2Seq)** model with **Bahdanau attention**, you can use pytorch, to learn how to **reverse word order in sentences**.\n",
        "\n",
        "> Example task:  \n",
        "> **Input:** `\"the cat sat\"`  \n",
        "> **Output:** `\"sat cat the\"`\n",
        "\n",
        "---\n",
        "\n",
        "## Part 1 — Model Architecture\n",
        "\n",
        "### Requirements\n",
        "1. **Encoder**\n",
        "   - Implement a GRU-based encoder.  \n",
        "   - Input: tokenized source sentence.  \n",
        "   - Output: sequence of hidden states.\n",
        "\n",
        "2. **Attention Mechanism (Bahdanau)**\n",
        "   - Compute alignment scores between the current decoder hidden state and all encoder outputs.  \n",
        "   - Apply softmax to get attention weights.  \n",
        "   - Derive a context vector as the weighted sum of encoder outputs.\n",
        "\n",
        "3. **Decoder**\n",
        "   - Implement a GRU decoder that uses the context vector at each step.  \n",
        "   - Predicts the next word in the reversed sequence.\n",
        "\n",
        "---\n",
        "\n",
        "## Part 2 — Training Loop\n",
        "\n",
        "### Requirements\n",
        "Implement a full **training loop** that includes:\n",
        "\n",
        "- **Loss:** Cross-entropy loss with padding mask (ignore padded tokens).  \n",
        "- **Optimization:** Implement **Adam optimizer** manually.  \n",
        "- **Gradient Clipping:** Apply **max-norm clipping** (norm ≤ 1.0).  \n",
        "- **Teacher Forcing:** Use teacher forcing during training.  \n",
        "- **Model Saving:** Save the best model based on validation loss.  \n",
        "- **Logging:** Print training and validation loss for each epoch.\n",
        "\n",
        "---\n",
        "\n",
        "## Part 3 — Evaluation & Visualization\n",
        "\n",
        "After training, evaluate the model on a test set and report:\n",
        "\n",
        "1. **Qualitative Examples**\n",
        "   Show at least **10 examples** in the following format:\n",
        "   Input: \"the cat sat\"\n",
        "   Output: \"sat cat the\"\n",
        "   Reference: \"sat cat the\"\n",
        "   match or no match\n",
        "\n",
        "2. **Quantitative Metric**\n",
        "- Compute **exact match accuracy** across the test set.\n",
        "\n",
        "3. **Attention Visualization**\n",
        "- Plot a **heatmap** showing attention weights.  \n",
        "- X-axis → encoder tokens  \n",
        "- Y-axis → decoder steps  \n",
        "- Save as `attention_heatmap.png`\n",
        "\n",
        "---\n",
        "\n",
        "## Part 4 — Analysis\n",
        "\n",
        "Write a short answering:\n",
        "\n",
        "- What patterns do you observe in the attention weights?  \n",
        "- Does the attention align input and output tokens correctly?  \n",
        "- How does attention help the model learn to reverse sequences?  \n",
        "- What happens at the beginning and end of sequences?\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ZVXt9kbM6ybh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AJtn23n-65qF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
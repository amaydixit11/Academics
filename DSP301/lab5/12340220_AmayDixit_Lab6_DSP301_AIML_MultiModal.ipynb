{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "50bf5a252b504308829a53425e4d4d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1476bd77d8e54c3788d3ca315f29667a",
              "IPY_MODEL_52f2f59e698d40e0940bdd55133ee9de",
              "IPY_MODEL_b09845f567d142b4945e61ebbd492cea"
            ],
            "layout": "IPY_MODEL_f8e1fa854e544e4eb9c0abefb681133d"
          }
        },
        "1476bd77d8e54c3788d3ca315f29667a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43020fc9e1e54bbb9c923e5ff5d63a8d",
            "placeholder": "​",
            "style": "IPY_MODEL_56c08d55dde64253b354165e4bfe2e37",
            "value": "Fetching 1 files: 100%"
          }
        },
        "52f2f59e698d40e0940bdd55133ee9de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7189a87f2b2a4098af2527d79a74bd4d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f54d825ab69d42c992bf93210f05fd6f",
            "value": 1
          }
        },
        "b09845f567d142b4945e61ebbd492cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9846fa18ba5f4c7ea82ce2f14f863c48",
            "placeholder": "​",
            "style": "IPY_MODEL_89f14c26b0ed421eaf9ce6cc456a9d6a",
            "value": " 1/1 [00:00&lt;00:00, 42.43it/s]"
          }
        },
        "f8e1fa854e544e4eb9c0abefb681133d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43020fc9e1e54bbb9c923e5ff5d63a8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56c08d55dde64253b354165e4bfe2e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7189a87f2b2a4098af2527d79a74bd4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f54d825ab69d42c992bf93210f05fd6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9846fa18ba5f4c7ea82ce2f14f863c48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89f14c26b0ed421eaf9ce6cc456a9d6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZIXi0bfLGaB7",
        "outputId": "6e537fe1-155f-4fa6-8112-c4d4c3c6e27f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.12/dist-packages (0.10.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.1.12)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.25.8)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers sentence-transformers scikit-learn pandas opencv-python moviepy mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "from moviepy.editor import VideoFileClip\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "JwpcFhZEHVsh"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section initializes and loads the three core AI models that form the backbone of our multimodal system. Each model is responsible for a different modality: speech, vision, and language.\n",
        "1.  **Whisper**: A state-of-the-art speech-to-text model from OpenAI for transcribing spoken words.\n",
        "2.  **MediaPipe Hands**: A computer vision model from Google for detecting hand landmarks in real-time.\n",
        "3.  **Zero-Shot Classifier**: A powerful NLP model (BART) that can classify text into predefined categories (intents) without being explicitly trained on them.\n",
        "Using a GPU (`device=0`) is specified to significantly speed up model inference."
      ],
      "metadata": {
        "id": "OnJ-PykprSnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Speech-to-Text Model (Whisper)\n",
        "# Using a GPU (device=0) is highly recommended for Whisper\n",
        "stt_pipeline = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\", device=0)\n",
        "print(\"--> Whisper Speech-to-Text model loaded.\")\n",
        "\n",
        "# 2. Hand Gesture Model (MediaPipe)\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "print(\"--> MediaPipe Hand Gesture model loaded.\")\n",
        "\n",
        "# 3. ZERO-SHOT TEXT-TO-INTENT NLP Model\n",
        "# We replace our custom classifier with a powerful pre-trained model.\n",
        "# facebook/bart-large-mnli is a popular choice for this task.\n",
        "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)\n",
        "# Define our possible intents which will be the candidate labels\n",
        "CANDIDATE_INTENTS = [\"forward\", \"left\", \"right\", \"stop\"]\n",
        "print(\"--> Zero-Shot Intent NLP model loaded.\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\nAll models are ready.\\n\" + \"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "50bf5a252b504308829a53425e4d4d51",
            "1476bd77d8e54c3788d3ca315f29667a",
            "52f2f59e698d40e0940bdd55133ee9de",
            "b09845f567d142b4945e61ebbd492cea",
            "f8e1fa854e544e4eb9c0abefb681133d",
            "43020fc9e1e54bbb9c923e5ff5d63a8d",
            "56c08d55dde64253b354165e4bfe2e37",
            "7189a87f2b2a4098af2527d79a74bd4d",
            "f54d825ab69d42c992bf93210f05fd6f",
            "9846fa18ba5f4c7ea82ce2f14f863c48",
            "89f14c26b0ed421eaf9ce6cc456a9d6a"
          ]
        },
        "collapsed": true,
        "id": "986FZ_BmH-ha",
        "outputId": "74db89ca-ff09-43e8-9f9d-d234282f8ffb"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50bf5a252b504308829a53425e4d4d51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Whisper Speech-to-Text model loaded.\n",
            "--> MediaPipe Hand Gesture model loaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Zero-Shot Intent NLP model loaded.\n",
            "\n",
            "==================================================\n",
            "All models are ready.\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes a string of text (the transcript from the audio) and uses the pre-trained zero-shot classification model to determine which of the `CANDIDATE_INTENTS` it most closely matches. It works \"zero-shot,\" meaning the model was not specifically trained on our \"forward,\" \"left,\" \"right,\" or \"stop\" commands but can generalize to understand them. The function only returns an intent if the model's confidence score exceeds a specified threshold, preventing uncertain classifications."
      ],
      "metadata": {
        "id": "1b5ELIO_stVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_intent_from_text_zero_shot(transcript, confidence_threshold=0.60):\n",
        "    \"\"\"\n",
        "    Classifies a text command into an intent using a zero-shot model.\n",
        "    \"\"\"\n",
        "    if not transcript:\n",
        "        return None\n",
        "\n",
        "    print(f\"[NLP] Classifying text: '{transcript}'\")\n",
        "\n",
        "    # The model returns scores for all candidate labels, sorted from highest to lowest.\n",
        "    results = zero_shot_classifier(transcript, CANDIDATE_INTENTS)\n",
        "\n",
        "    best_intent = results['labels'][0]\n",
        "    best_score = results['scores'][0]\n",
        "\n",
        "    print(f\"[NLP] Top classification: '{best_intent}' with confidence: {best_score:.2f}\")\n",
        "\n",
        "    # Only return the intent if the model is confident enough\n",
        "    if best_score > confidence_threshold:\n",
        "        print(f\"[NLP] Confidence is above threshold. Intent is '{best_intent}'.\")\n",
        "        return best_intent\n",
        "    else:\n",
        "        print(f\"[NLP] Confidence is below threshold. Intent is uncertain.\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "UAQMiZKcIjyN"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It takes the path to an audio file, uses the Whisper model to transcribe the speech into text, and then passes this text to our `get_intent_from_text_zero_shot` function to determine the final command intent. It includes error handling in case the audio processing fails."
      ],
      "metadata": {
        "id": "nZgk6U5ss6EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_intent_from_audio(audio_path):\n",
        "    \"\"\"\n",
        "    Takes an audio file path, transcribes it, and classifies the intent using the zero-shot model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"\\n[Audio] Transcribing speech to text...\")\n",
        "        transcription_result = stt_pipeline(audio_path)\n",
        "        transcript = transcription_result['text'].strip().lower()\n",
        "\n",
        "        # We now call our new zero-shot function\n",
        "        return get_intent_from_text_zero_shot(transcript)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[Audio] Error processing audio: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "EADFgMtdImSf"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function handles the visual modality. It analyzes a video file frame by frame to identify hand gestures. It uses MediaPipe to detect hand landmarks (the positions of joints) and then applies a set of geometric rules to recognize specific gestures: a fist with an extended thumb (for \"left\" or \"right\"), an open palm (\"stop\"), and a thumbs-up (\"forward\"). To make the detection robust, it counts the occurrences of each gesture throughout the video and returns the most frequently seen (dominant) gesture, as long as it's detected a minimum number of times."
      ],
      "metadata": {
        "id": "HihjGvrttCmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a finger to be curled, its tip must be \"lower\" on the screen than its middle joint (the PIP joint). In screen coordinates, a higher y value means lower on the screen. This condition checks if the main fingers are bent downwards."
      ],
      "metadata": {
        "id": "NK3ijjdg7Feq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_intent_from_video(video_path):\n",
        "    \"\"\"\n",
        "    Analyzes a video for hand gestures using a prioritized check:\n",
        "    1. Fist w/ Thumb (Left/Right)\n",
        "    2. Open Palm (Stop)\n",
        "    3. Thumbs Up (Forward)\n",
        "    \"\"\"\n",
        "    print(\"\\n[Video] Analyzing video for hand gestures...\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened(): return None\n",
        "\n",
        "    gesture_counts = {\"left\": 0, \"right\": 0, \"forward\": 0, \"stop\": 0, \"unknown\": 0}\n",
        "    frame_count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "\n",
        "        if frame_count % 5 == 0:\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            results = hands.process(frame_rgb)\n",
        "\n",
        "            if results.multi_hand_landmarks:\n",
        "                for hand_landmarks in results.multi_hand_landmarks:\n",
        "                    # Collect key landmarks\n",
        "                    # Collect key landmarks\n",
        "                    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
        "                    thumb_ip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_IP]\n",
        "\n",
        "                    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
        "                    index_pip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_PIP]\n",
        "\n",
        "                    middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
        "                    middle_pip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_PIP]\n",
        "\n",
        "                    ring_tip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP]\n",
        "                    ring_pip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_PIP]\n",
        "\n",
        "                    pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]\n",
        "                    pinky_pip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_PIP]\n",
        "\n",
        "                    wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
        "\n",
        "                    # 1. Condition for Left/Right Fist\n",
        "                    index_folded = index_tip.y > index_pip.y\n",
        "                    middle_folded = middle_tip.y > middle_pip.y\n",
        "                    ring_folded = ring_tip.y > ring_pip.y\n",
        "                    pinky_folded = pinky_tip.y > pinky_pip.y\n",
        "\n",
        "                    is_fist_with_thumb = (\n",
        "                        index_folded and middle_folded and ring_folded and pinky_folded and\n",
        "                        abs(thumb_tip.y - thumb_ip.y) < 0.05\n",
        "                    )\n",
        "\n",
        "                    # 2. Condition for Stop (Open Palm)\n",
        "                    fingers_open = (\n",
        "                        index_tip.y < index_pip.y and\n",
        "                        middle_tip.y < middle_pip.y and\n",
        "                        ring_tip.y < ring_pip.y and\n",
        "                        pinky_tip.y < pinky_pip.y and\n",
        "                        thumb_tip.y < thumb_ip.y\n",
        "                    )\n",
        "\n",
        "                    # 3. Condition for Forward (Thumbs Up)\n",
        "                    is_thumbs_up = (\n",
        "                        thumb_tip.y < thumb_ip.y - 0.03 and\n",
        "                        index_folded and middle_folded and ring_folded and pinky_folded\n",
        "                    )\n",
        "\n",
        "                    # PRIORITY 1: Check for Left/Right Fist\n",
        "                    if is_fist_with_thumb:\n",
        "\n",
        "                        if thumb_tip.x < wrist.x - 0.04:\n",
        "                            gesture_counts[\"left\"] += 1\n",
        "                        elif thumb_tip.x > wrist.x + 0.04:\n",
        "                            gesture_counts[\"right\"] += 1\n",
        "                        else: # Could be a thumbs up, check in the next step\n",
        "                            if is_thumbs_up:\n",
        "                                gesture_counts[\"forward\"] += 1\n",
        "                            else:\n",
        "                                gesture_counts[\"unknown\"] += 1\n",
        "\n",
        "                    # PRIORITY 2: Check for Stop (Open Palm)\n",
        "                    elif fingers_open:\n",
        "                        gesture_counts[\"stop\"] += 1\n",
        "\n",
        "                    # PRIORITY 3: Check for Forward (Thumbs Up) if not caught by fist logic\n",
        "                    elif is_thumbs_up:\n",
        "                        gesture_counts[\"forward\"] += 1\n",
        "\n",
        "                    # FALLBACK\n",
        "                    else:\n",
        "                        gesture_counts[\"unknown\"] += 1\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if sum(gesture_counts.values()) > 0:\n",
        "        dominant_gesture = max(gesture_counts, key=gesture_counts.get)\n",
        "        if dominant_gesture != \"unknown\" and gesture_counts[dominant_gesture] > 2:\n",
        "             print(f\"[Video] Detected Gesture Counts: {gesture_counts}\")\n",
        "             print(f\"[Video] Detected Intent: '{dominant_gesture}'\")\n",
        "             return dominant_gesture\n",
        "\n",
        "    print(\"[Video] No definitive gesture detected.\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "FIIcSyVJIqaY"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the core function that combines the entire multimodal analysis. It takes a video file path as input and performs the following steps:\n",
        "1.  Extracts the audio from the video into a temporary file.\n",
        "2.  Runs the audio processing pipeline to get an `audio_intent`.\n",
        "3.  Runs the video gesture recognition pipeline to get a `video_intent`.\n",
        "4.  Decision"
      ],
      "metadata": {
        "id": "zZLFH7mqtKEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_multimodal_command(video_path):\n",
        "    \"\"\"\n",
        "    The main pipeline function with updated, more flexible decision logic.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*20} PROCESSING NEW COMMAND: {video_path} {'='*20}\")\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"Error: Video file not found at {video_path}\"); return\n",
        "\n",
        "    # --- Step 1: Extract Audio & Get Intents ---\n",
        "    temp_audio_path = \"temp_audio.wav\"\n",
        "    try:\n",
        "        with VideoFileClip(video_path) as video_clip:\n",
        "            video_clip.audio.write_audiofile(temp_audio_path, logger=None)\n",
        "        audio_intent = get_intent_from_audio(temp_audio_path)\n",
        "    except Exception:\n",
        "        audio_intent = None # Assume no audio if extraction fails\n",
        "    finally:\n",
        "        if os.path.exists(temp_audio_path): os.remove(temp_audio_path)\n",
        "\n",
        "    video_intent = get_intent_from_video(video_path)\n",
        "\n",
        "    # --- Step 2: NEW DECISION LOGIC ---\n",
        "    print(\"\\n[Fusion] Comparing intents...\")\n",
        "    print(f\"[Fusion] Audio Intent: {audio_intent} | Video Intent: {video_intent}\")\n",
        "\n",
        "    # Case 1: High confidence match\n",
        "    if audio_intent and video_intent and audio_intent == video_intent:\n",
        "        print(f\"\\nHIGH CONFIDENCE: Intents match! Executing command: {audio_intent.upper()}\")\n",
        "        # Your robot action call, e.g., move_robot(audio_intent)\n",
        "\n",
        "    # Case 2: Conflict\n",
        "    elif audio_intent and video_intent and audio_intent != video_intent:\n",
        "        print(f\"\\n CONFLICT: Audio detected '{audio_intent}' but Video detected '{video_intent}'. No action taken.\")\n",
        "\n",
        "    # Case 3: Audio only\n",
        "    elif audio_intent and not video_intent:\n",
        "        print(f\"\\n AUDIO ONLY: Proceeding with audio command: {audio_intent.upper()}\")\n",
        "        # Your robot action call, e.g., move_robot(audio_intent)\n",
        "\n",
        "    # Case 4: Video only\n",
        "    elif video_intent and not audio_intent:\n",
        "        print(f\"\\n VIDEO ONLY: Proceeding with video command: {video_intent.upper()}\")\n",
        "        # Your robot action call, e.g., move_robot(video_intent)\n",
        "\n",
        "    # Case 5: No intent detected\n",
        "    else: # This covers the case where both are None\n",
        "        print(\"\\nFAILED: No clear audio or video intent was detected. Please try again.\")\n"
      ],
      "metadata": {
        "id": "WwNOl5PbIvY2"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    test_videos = [\n",
        "        \"/content/right_3.mp4\",\n",
        "        \"/content/left_3.mp4\",\n",
        "        \"/content/forward_3.mp4\",\n",
        "        \"/content/stop_3.mp4\",\n",
        "    ]\n",
        "\n",
        "    for video_file in test_videos:\n",
        "        process_multimodal_command(video_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4svpSo_pG6s",
        "outputId": "3e51f634-89f7-4a6c-d4f8-86f41731f641"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== PROCESSING NEW COMMAND: /content/right_3.mp4 ====================\n",
            "\n",
            "[Audio] Transcribing speech to text...\n",
            "[NLP] Classifying text: 'great.'\n",
            "[NLP] Top classification: 'right' with confidence: 0.51\n",
            "[NLP] Confidence is below threshold. Intent is uncertain.\n",
            "\n",
            "[Video] Analyzing video for hand gestures...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Video] Detected Gesture Counts: {'left': 0, 'right': 15, 'forward': 0, 'stop': 0, 'unknown': 0}\n",
            "[Video] Detected Intent: 'right'\n",
            "\n",
            "[Fusion] Comparing intents...\n",
            "[Fusion] Audio Intent: None | Video Intent: right\n",
            "\n",
            " VIDEO ONLY: Proceeding with video command: RIGHT\n",
            "\n",
            "==================== PROCESSING NEW COMMAND: /content/left_3.mp4 ====================\n",
            "\n",
            "[Audio] Transcribing speech to text...\n",
            "[NLP] Classifying text: 'deaf.'\n",
            "[NLP] Top classification: 'left' with confidence: 0.59\n",
            "[NLP] Confidence is below threshold. Intent is uncertain.\n",
            "\n",
            "[Video] Analyzing video for hand gestures...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Video] Detected Gesture Counts: {'left': 12, 'right': 0, 'forward': 0, 'stop': 0, 'unknown': 0}\n",
            "[Video] Detected Intent: 'left'\n",
            "\n",
            "[Fusion] Comparing intents...\n",
            "[Fusion] Audio Intent: None | Video Intent: left\n",
            "\n",
            " VIDEO ONLY: Proceeding with video command: LEFT\n",
            "\n",
            "==================== PROCESSING NEW COMMAND: /content/forward_3.mp4 ====================\n",
            "\n",
            "[Audio] Transcribing speech to text...\n",
            "[NLP] Classifying text: 'fudburg.'\n",
            "[NLP] Top classification: 'forward' with confidence: 0.50\n",
            "[NLP] Confidence is below threshold. Intent is uncertain.\n",
            "\n",
            "[Video] Analyzing video for hand gestures...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Video] No definitive gesture detected.\n",
            "\n",
            "[Fusion] Comparing intents...\n",
            "[Fusion] Audio Intent: None | Video Intent: None\n",
            "\n",
            "FAILED: No clear audio or video intent was detected. Please try again.\n",
            "\n",
            "==================== PROCESSING NEW COMMAND: /content/stop_3.mp4 ====================\n",
            "\n",
            "[Audio] Transcribing speech to text...\n",
            "[NLP] Classifying text: 'stop.'\n",
            "[NLP] Top classification: 'stop' with confidence: 0.65\n",
            "[NLP] Confidence is above threshold. Intent is 'stop'.\n",
            "\n",
            "[Video] Analyzing video for hand gestures...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Video] Detected Gesture Counts: {'left': 3, 'right': 0, 'forward': 0, 'stop': 11, 'unknown': 0}\n",
            "[Video] Detected Intent: 'stop'\n",
            "\n",
            "[Fusion] Comparing intents...\n",
            "[Fusion] Audio Intent: stop | Video Intent: stop\n",
            "\n",
            "HIGH CONFIDENCE: Intents match! Executing command: STOP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ZyvbUDhuErF"
      },
      "execution_count": 183,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "790d2111-1097-4f68-a15a-f79ef3b18294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import gc\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "# Option 1: Add PyG classes to safe globals\n",
    "torch.serialization.add_safe_globals([\n",
    "    'torch_geometric.data.data.Data',\n",
    "    'torch_geometric.data.data.DataEdgeAttr',\n",
    "    'torch_geometric.data.storage.EdgeStorage',\n",
    "    'torch_geometric.data.storage.NodeStorage'\n",
    "])\n",
    "\n",
    "# Modified load function\n",
    "def load_pyg_graph(input_dir=r\"C:\\Users\\IIT BHILAI\\Desktop\\Reddit SNA\\cleaned_graph_data\"):\n",
    "    \"\"\"Load the PyG graph and user mapping from disk\"\"\"\n",
    "    # Load the graph data with weights_only=False or use the safer approach above\n",
    "    try:\n",
    "        \n",
    "        # Try with safe globals (preferred approach)\n",
    "        data = torch.load(os.path.join(input_dir, \"cleaned_user_graph.pt\"))\n",
    "    except Exception as e:\n",
    "        # Fallback option - less secure but will work\n",
    "        print(\"Using fallback loading method...\")\n",
    "        data = torch.load(os.path.join(input_dir, \"cleaned_user_graph.pt\"), weights_only=False)\n",
    "    \n",
    "    # Load the user mapping\n",
    "    user_to_idx = {}\n",
    "    idx_to_user = {}\n",
    "    \n",
    "    with h5py.File(os.path.join(input_dir, \"cleaned_user_mapping.h5\"), 'r') as f:\n",
    "        users = [u.decode('utf-8') for u in f['users'][:]]\n",
    "        indices = f['indices'][:]\n",
    "        \n",
    "        for user, idx in zip(users, indices):\n",
    "            user_to_idx[user] = idx\n",
    "            idx_to_user[idx] = user\n",
    "    \n",
    "    return data, user_to_idx, idx_to_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ed3811-710f-4319-9b62-71638d45a541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fallback loading method...\n"
     ]
    }
   ],
   "source": [
    "data, user_to_idx, idx_to_user = load_pyg_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a114ccc-60d4-4935-b241-41b6239b8f3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.utils import homophily, k_hop_subgraph\n",
    "from torch_geometric.utils import degree, to_scipy_sparse_matrix\n",
    "from torch_geometric.transforms import AddLaplacianEigenvectorPE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import scipy.stats as stats\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import community.community_louvain as community_louvain\n",
    "import networkx as nx  # Still needed for some algorithms but used minimally\n",
    "import torch_geometric.nn as pyg_nn\n",
    "# from torch_sparse import coalesce\n",
    "from torch_geometric.utils import remove_self_loops\n",
    "# import torch_cluster\n",
    "\n",
    "# 1. Basic Graph Analysis\n",
    "\n",
    "def compute_basic_stats(data):\n",
    "    \"\"\"Compute basic statistics without using NetworkX\"\"\"\n",
    "    print(f\"Number of nodes: {data.num_nodes}\")\n",
    "    print(f\"Number of edges: {data.num_edges // 2} (undirected)\")\n",
    "    \n",
    "    # Calculate degree distribution\n",
    "    row, col = data.edge_index\n",
    "    degrees = degree(row, num_nodes=data.num_nodes)\n",
    "    \n",
    "    # Average degree\n",
    "    avg_degree = degrees.float().mean().item()\n",
    "    print(f\"Average degree: {avg_degree:.2f}\")\n",
    "    \n",
    "    # Max degree and node with max degree\n",
    "    max_degree = degrees.max().item()\n",
    "    max_degree_node = degrees.argmax().item()\n",
    "    print(f\"Max degree: {max_degree} (Node {max_degree_node}: {idx_to_user[max_degree_node]})\")\n",
    "    \n",
    "    # Degree distribution statistics\n",
    "    degree_values = degrees.numpy()\n",
    "    print(f\"Degree distribution stats: Min={degree_values.min()}, Median={np.median(degree_values)}, Mean={degree_values.mean():.2f}\")\n",
    "    \n",
    "    # Calculate degree percentiles\n",
    "    percentiles = [25, 50, 75, 90, 95, 99]\n",
    "    degree_percentiles = np.percentile(degree_values, percentiles)\n",
    "    for p, val in zip(percentiles, degree_percentiles):\n",
    "        print(f\"{p}th percentile: {val:.2f}\")\n",
    "    \n",
    "    return degrees\n",
    "\n",
    "\n",
    "def find_influential_users(data, degrees, user_mapping, top_n=20):\n",
    "    \"\"\"Find the most influential users in the graph\"\"\"\n",
    "    idx_to_user = user_mapping\n",
    "    \n",
    "    # Get top users by degree\n",
    "    top_indices = torch.argsort(degrees, descending=True)[:top_n].tolist()\n",
    "    \n",
    "    print(f\"\\nTop {top_n} users by connections:\")\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        print(f\"{i+1}. {idx_to_user[idx]}: {degrees[idx].item()} connections\")\n",
    "    \n",
    "    return top_indices\n",
    "\n",
    "\n",
    "def extremely_safe_analyze_degrees(degrees, chunk_size=10000):\n",
    "    \"\"\"\n",
    "    Extremely safe degree analysis that processes data in small chunks\n",
    "    to avoid memory issues\n",
    "    \"\"\"\n",
    "    print(\"Starting extremely safe degree analysis...\")\n",
    "    \n",
    "    # Convert tensor to list in chunks to avoid memory issues\n",
    "    degree_list = []\n",
    "    total_nodes = degrees.shape[0]\n",
    "    \n",
    "    for i in range(0, total_nodes, chunk_size):\n",
    "        end = min(i + chunk_size, total_nodes)\n",
    "        chunk = degrees[i:end].tolist()\n",
    "        degree_list.extend(chunk)\n",
    "        print(f\"Processed chunk {i//chunk_size + 1}/{(total_nodes + chunk_size - 1)//chunk_size}\")\n",
    "        # Force garbage collection after each chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"Basic statistics calculation...\")\n",
    "    # Basic statistics - calculated manually to avoid numpy memory issues\n",
    "    min_degree = min(degree_list)\n",
    "    max_degree = max(degree_list)\n",
    "    \n",
    "    sum_degrees = 0\n",
    "    for d in degree_list:\n",
    "        sum_degrees += d\n",
    "    mean_degree = sum_degrees / len(degree_list)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Total nodes: {len(degree_list)}\")\n",
    "    print(f\"Min degree: {min_degree}\")\n",
    "    print(f\"Max degree: {max_degree}\")\n",
    "    print(f\"Mean degree: {mean_degree:.2f}\")\n",
    "    \n",
    "    # Count degrees in buckets to create a simple histogram without plotting\n",
    "    print(\"Creating degree frequency table...\")\n",
    "    degree_counts = {}\n",
    "    \n",
    "    # Process in chunks to avoid memory issues\n",
    "    for i in range(0, len(degree_list), chunk_size):\n",
    "        chunk = degree_list[i:min(i+chunk_size, len(degree_list))]\n",
    "        for d in chunk:\n",
    "            if d in degree_counts:\n",
    "                degree_counts[d] += 1\n",
    "            else:\n",
    "                degree_counts[d] = 1\n",
    "        print(f\"Processed count chunk {i//chunk_size + 1}/{(len(degree_list) + chunk_size - 1)//chunk_size}\")\n",
    "        gc.collect()\n",
    "    \n",
    "    # Save degree counts to a text file\n",
    "    print(\"Saving degree counts to file...\")\n",
    "    with open(\"degree_counts.txt\", \"w\") as f:\n",
    "        f.write(\"Degree,Count\\n\")\n",
    "        for d in sorted(degree_counts.keys()):\n",
    "            f.write(f\"{d},{degree_counts[d]}\\n\")\n",
    "    \n",
    "    print(\"Degree count data saved to degree_counts.txt\")\n",
    "    print(\"You can use this file later to create visualizations without memory issues\")\n",
    "    \n",
    "    # Return simple summary stats\n",
    "    return {\n",
    "        \"nodes\": len(degree_list),\n",
    "        \"min_degree\": min_degree,\n",
    "        \"max_degree\": max_degree,\n",
    "        \"mean_degree\": mean_degree,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e822bb-bbc8-457c-a397-c1b099966521",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_random_subgraph(data, num_nodes=500, seed=42):\n",
    "    \"\"\"\n",
    "    Sample a random subgraph for visualization\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Sample random nodes\n",
    "    sampled_nodes = np.random.choice(data.num_nodes, size=min(num_nodes, data.num_nodes), replace=False)\n",
    "    sampled_nodes = torch.tensor(sampled_nodes)\n",
    "    \n",
    "    # Extract the subgraph\n",
    "    row, col = data.edge_index\n",
    "    mask = torch.isin(row, sampled_nodes) & torch.isin(col, sampled_nodes)\n",
    "    sub_edge_index = data.edge_index[:, mask]\n",
    "    \n",
    "    # Create node mapping\n",
    "    node_map = {int(node): i for i, node in enumerate(sampled_nodes)}\n",
    "    \n",
    "    # Relabel nodes\n",
    "    sub_row = torch.tensor([node_map[int(node)] for node in sub_edge_index[0]])\n",
    "    sub_col = torch.tensor([node_map[int(node)] for node in sub_edge_index[1]])\n",
    "    sub_edge_index = torch.stack([sub_row, sub_col])\n",
    "    \n",
    "    # Calculate node degrees\n",
    "    sub_degrees = degree(sub_edge_index[0], num_nodes=len(sampled_nodes))\n",
    "    \n",
    "    # Create a simple NetworkX graph for layout\n",
    "    G = nx.Graph()\n",
    "    for i in range(len(sampled_nodes)):\n",
    "        G.add_node(i)\n",
    "    for i in range(sub_edge_index.shape[1]):\n",
    "        G.add_edge(sub_edge_index[0, i].item(), sub_edge_index[1, i].item())\n",
    "    \n",
    "    # Calculate layout\n",
    "    pos = nx.spring_layout(G, seed=seed)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    # Plot edges\n",
    "    for i, j in zip(sub_edge_index[0], sub_edge_index[1]):\n",
    "        i, j = i.item(), j.item()\n",
    "        plt.plot([pos[i][0], pos[j][0]], [pos[i][1], pos[j][1]], 'k-', alpha=0.2, linewidth=0.5)\n",
    "    \n",
    "    # Plot nodes\n",
    "    node_sizes = 10 + 50 * (sub_degrees / max(sub_degrees.max(), 1))\n",
    "    for i in range(len(sampled_nodes)):\n",
    "        plt.scatter(pos[i][0], pos[i][1], s=node_sizes[i].item(), \n",
    "                    c=plt.cm.viridis(float(sub_degrees[i])/max(sub_degrees.max(), 1)), \n",
    "                    alpha=0.7)\n",
    "    \n",
    "    plt.title(f'Random Subgraph with {len(sampled_nodes)} Nodes')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output_123/random_subgraph.png', dpi=300)\n",
    "    plt.close()  # Close to free memory\n",
    "    print(\"Random subgraph visualization saved to 'random_subgraph.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2661c05-a352-4e4a-b7a7-6c34913b5708",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_homophily(data, node_features, feature_name=\"feature\"):\n",
    "    \"\"\"\n",
    "    Analyze homophily in the graph with respect to a node feature\n",
    "    \"\"\"\n",
    "    h_score = homophily(data.edge_index, node_features, method='edge')\n",
    "    print(f\"Homophily score for {feature_name}: {h_score:.4f}\")\n",
    "    \n",
    "    # Calculate feature distribution\n",
    "    unique_values, counts = torch.unique(node_features, return_counts=True)\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(unique_values.numpy(), counts.numpy())\n",
    "    plt.xlabel(feature_name)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Distribution of {feature_name} Values')\n",
    "    plt.savefig(f'{feature_name}_distribution.png', dpi=300)\n",
    "    plt.close()  # Close to free memory\n",
    "    \n",
    "    return h_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16f20205-150c-40e7-a146-336117b6cdf9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_and_visualize_graph(data, user_mapping=None):\n",
    "    \"\"\"\n",
    "    Full analysis pipeline with memory-efficient visualization\n",
    "    \"\"\"\n",
    "    # Import gc to help with garbage collection\n",
    "    import gc\n",
    "    \n",
    "    # Compute degrees (this is required for most analyses)\n",
    "    print(\"Computing node degrees...\")\n",
    "    row, col = data.edge_index\n",
    "    degrees = degree(row, num_nodes=data.num_nodes)\n",
    "    \n",
    "    # Save degrees to a file for later use\n",
    "    print(\"Saving degree data to file...\")\n",
    "    with open(\"degree_data.txt\", \"w\") as f:\n",
    "        for i, d in enumerate(degrees):\n",
    "            f.write(f\"{i},{d.item()}\\n\")\n",
    "    \n",
    "    # Create degree counts file (for plotting later) - without using bincount\n",
    "    print(\"Creating degree counts file...\")\n",
    "    # Convert to int if it's float\n",
    "    if degrees.dtype.is_floating_point:\n",
    "        degrees_int = degrees.long()\n",
    "    else:\n",
    "        degrees_int = degrees\n",
    "        \n",
    "    # Count frequencies manually to avoid memory issues\n",
    "    degree_count_dict = {}\n",
    "    for d in degrees_int:\n",
    "        d_val = d.item()\n",
    "        if d_val in degree_count_dict:\n",
    "            degree_count_dict[d_val] += 1\n",
    "        else:\n",
    "            degree_count_dict[d_val] = 1\n",
    "    \n",
    "    # Write to file\n",
    "    with open(\"degree_counts.txt\", \"w\") as f:\n",
    "        f.write(\"Degree,Count\\n\")\n",
    "        for d in sorted(degree_count_dict.keys()):\n",
    "            f.write(f\"{d},{degree_count_dict[d]}\\n\")\n",
    "    \n",
    "    # Plot degree distribution from file (memory efficient)\n",
    "    print(\"Plotting degree distribution...\")\n",
    "    plot_degree_distribution(\"degree_counts.txt\", log_scale=True)\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Plot CCDF\n",
    "    print(\"Plotting CCDF...\")\n",
    "    plot_ccdf(degrees)\n",
    "    gc.collect()\n",
    "    \n",
    "    # Get top users if user mapping exists\n",
    "    if user_mapping:\n",
    "        top_users = find_influential_users(data, degrees, user_mapping, top_n=20)\n",
    "        \n",
    "        # Visualize ego network of the most connected user\n",
    "        print(\"Visualizing ego network of the most connected user...\")\n",
    "        visualize_ego_network(data, top_users[0], hops=1)\n",
    "        gc.collect()\n",
    "    \n",
    "    # Visualize a random subgraph (very memory efficient)\n",
    "    print(\"Visualizing random subgraph...\")\n",
    "    visualize_random_subgraph(data, num_nodes=500)\n",
    "    gc.collect()\n",
    "    \n",
    "    # Calculate connected components (this can be memory intensive for very large graphs)\n",
    "    print(\"Calculating connected components...\")\n",
    "    try:\n",
    "        visualize_connected_components_sizes(data)\n",
    "    except MemoryError:\n",
    "        print(\"Memory error when calculating connected components - skipping this step\")\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40854fb1-6ce9-440c-a724-750d35d7bb58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2. Community Detection and Network Structure\n",
    "\n",
    "def detect_communities(data, method=\"louvain\", resolution=1.0, max_nodes=100000):\n",
    "    \"\"\"Detect communities in the graph using different methods\"\"\"\n",
    "    print(\"\\nPerforming community detection...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # For very large graphs, we might need to sample\n",
    "    if data.num_nodes > max_nodes:\n",
    "        print(f\"Graph is large, sampling {max_nodes} nodes for community detection...\")\n",
    "        sample_start = time.time()\n",
    "        # Sample nodes (prefers higher degree nodes with some randomness)\n",
    "        degrees = degree(data.edge_index[0], num_nodes=data.num_nodes)\n",
    "        probs = degrees.float() / degrees.sum()\n",
    "        sampled_nodes = torch.multinomial(probs, max_nodes, replacement=False)\n",
    "        print(f\"Sampled {len(sampled_nodes)} nodes in {time.time() - sample_start:.2f} seconds\")\n",
    "        \n",
    "        # Extract subgraph\n",
    "        print(\"Extracting subgraph...\")\n",
    "        subgraph_start = time.time()\n",
    "        subset, edge_index, _, _ = k_hop_subgraph(\n",
    "            node_idx=sampled_nodes, \n",
    "            num_hops=1,\n",
    "            edge_index=data.edge_index, \n",
    "            relabel_nodes=True,\n",
    "            num_nodes=data.num_nodes\n",
    "        )\n",
    "        print(f\"Subgraph extraction completed in {time.time() - subgraph_start:.2f} seconds\")\n",
    "        print(f\"Subgraph has {len(subset)} nodes and {edge_index.shape[1]} edges\")\n",
    "        \n",
    "        # Create a new data object for the subgraph\n",
    "        sub_data = pyg.data.Data(edge_index=edge_index, num_nodes=len(subset))\n",
    "    else:\n",
    "        print(f\"Using full graph with {data.num_nodes} nodes and {data.edge_index.shape[1]} edges\")\n",
    "        sub_data = data\n",
    "        subset = torch.arange(data.num_nodes)\n",
    "    \n",
    "    # Convert to scipy sparse matrix for community detection\n",
    "    print(\"Converting to scipy sparse matrix...\")\n",
    "    sparse_start = time.time()\n",
    "    adj_sparse = to_scipy_sparse_matrix(sub_data.edge_index, num_nodes=sub_data.num_nodes)\n",
    "    print(f\"Conversion to sparse matrix completed in {time.time() - sparse_start:.2f} seconds\")\n",
    "    \n",
    "    # Convert to networkx for community detection algorithms\n",
    "    print(\"Converting to networkx graph...\")\n",
    "    nx_start = time.time()\n",
    "    G = nx.from_scipy_sparse_array(adj_sparse)\n",
    "    print(f\"Conversion to networkx completed in {time.time() - nx_start:.2f} seconds\")\n",
    "    print(f\"NetworkX graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "    \n",
    "    # Perform community detection\n",
    "    print(f\"Starting {method} community detection with resolution={resolution}...\")\n",
    "    comm_start = time.time()\n",
    "    \n",
    "    if method == \"louvain\":\n",
    "        # Use python-louvain for community detection\n",
    "        print(\"Running Louvain algorithm...\")\n",
    "        partition = community_louvain.best_partition(G, resolution=resolution)\n",
    "        print(f\"Louvain completed in {time.time() - comm_start:.2f} seconds\")\n",
    "        \n",
    "        communities = {}\n",
    "        for node, comm_id in partition.items():\n",
    "            if comm_id not in communities:\n",
    "                communities[comm_id] = []\n",
    "            communities[comm_id].append(node)\n",
    "        \n",
    "    elif method == \"label_propagation\":\n",
    "        # Label propagation is faster but less accurate\n",
    "        print(\"Running Label Propagation algorithm...\")\n",
    "        communities = {i: list(c) for i, c in enumerate(nx.algorithms.community.label_propagation_communities(G))}\n",
    "        print(f\"Label Propagation completed in {time.time() - comm_start:.2f} seconds\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown community detection method: {method}\")\n",
    "    \n",
    "    # Map back to original node indices if we sampled\n",
    "    if data.num_nodes > max_nodes:\n",
    "        print(\"Mapping sampled nodes back to original indices...\")\n",
    "        map_start = time.time()\n",
    "        orig_communities = {}\n",
    "        for comm_id, nodes in communities.items():\n",
    "            orig_communities[comm_id] = [subset[node].item() for node in nodes]\n",
    "        communities = orig_communities\n",
    "        print(f\"Mapping back completed in {time.time() - map_start:.2f} seconds\")\n",
    "    \n",
    "    # Print community statistics\n",
    "    sizes = [len(nodes) for nodes in communities.values()]\n",
    "    print(f\"Found {len(communities)} communities\")\n",
    "    print(f\"Average community size: {np.mean(sizes):.2f}\")\n",
    "    print(f\"Largest community size: {max(sizes)}\")\n",
    "    \n",
    "    # Analyze top communities\n",
    "    top_communities = sorted(communities.items(), key=lambda x: len(x[1]), reverse=True)[:5]\n",
    "    print(\"\\nTop 5 communities by size:\")\n",
    "    for i, (comm_id, nodes) in enumerate(top_communities):\n",
    "        print(f\"Community {i+1}: {len(nodes)} nodes\")\n",
    "    \n",
    "    print(f\"Total community detection completed in {time.time() - start_time:.2f} seconds\")\n",
    "    return communities, top_communities\n",
    "\n",
    "\n",
    "def analyze_top_communities(communities, top_communities, data, idx_to_user, degrees, top_n=5):\n",
    "    \"\"\"Analyze the characteristics of top communities\"\"\"\n",
    "    print(\"\\nAnalyzing top communities...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, (comm_id, nodes) in enumerate(top_communities[:top_n]):\n",
    "        print(f\"\\nAnalyzing Community {i+1} ({len(nodes)} nodes)...\")\n",
    "        comm_start = time.time()\n",
    "        \n",
    "        # Get top users in this community by degree\n",
    "        print(\"  Identifying top users by degree...\")\n",
    "        user_start = time.time()\n",
    "        community_degrees = [(node, degrees[node].item()) for node in nodes]\n",
    "        top_users = sorted(community_degrees, key=lambda x: x[1], reverse=True)[:10]\n",
    "        print(f\"  Top users identified in {time.time() - user_start:.2f} seconds\")\n",
    "        \n",
    "        print(\"  Top users in this community:\")\n",
    "        for j, (node, deg) in enumerate(top_users):\n",
    "            print(f\"    {j+1}. {idx_to_user[node]} (degree: {deg})\")\n",
    "        \n",
    "        # Calculate average degree within community\n",
    "        print(\"  Calculating average degree...\")\n",
    "        avg_start = time.time()\n",
    "        community_avg_degree = np.mean([degrees[node].item() for node in nodes])\n",
    "        print(f\"  Average degree in community: {community_avg_degree:.2f}\")\n",
    "        print(f\"  Average degree calculated in {time.time() - avg_start:.2f} seconds\")\n",
    "        \n",
    "        # Calculate density (edges within community / possible edges)\n",
    "        if len(nodes) > 1:\n",
    "            print(\"  Calculating community density...\")\n",
    "            density_start = time.time()\n",
    "            \n",
    "            # Get subgraph\n",
    "            node_tensor = torch.tensor(nodes)\n",
    "            row, col = data.edge_index\n",
    "            print(\"    Finding internal edges...\")\n",
    "            mask = torch.isin(row, node_tensor) & torch.isin(col, node_tensor)\n",
    "            internal_edges = mask.sum().item() // 2  # divide by 2 for undirected\n",
    "            possible_edges = len(nodes) * (len(nodes) - 1) // 2\n",
    "            density = internal_edges / possible_edges\n",
    "            print(f\"  Community density: {density:.5f}\")\n",
    "            print(f\"  Density calculated in {time.time() - density_start:.2f} seconds\")\n",
    "        else:\n",
    "            print(\"  Community has only one node\")\n",
    "        \n",
    "        print(f\"Community {i+1} analysis completed in {time.time() - comm_start:.2f} seconds\")\n",
    "    \n",
    "    print(f\"Total community analysis completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Add import at the top of your file\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "171ac6d5-3abf-4809-9882-c080dddff326",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3. Node Embeddings and Advanced Analysis\n",
    "\n",
    "def compute_node_embeddings(data, dim=128, num_iterations=10):\n",
    "    \"\"\"Compute node embeddings using Node2Vec\"\"\"\n",
    "    print(\"\\nComputing node embeddings...\")\n",
    "    \n",
    "    # Remove self loops if any exist\n",
    "    edge_index, _ = remove_self_loops(data.edge_index)\n",
    "    \n",
    "    # Use torch_geometric's Node2Vec\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = pyg_nn.models.Node2Vec(\n",
    "        edge_index=edge_index,\n",
    "        embedding_dim=dim,\n",
    "        walk_length=10,\n",
    "        context_size=5,\n",
    "        walks_per_node=5,\n",
    "        num_negative_samples=1,\n",
    "        sparse=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Train model\n",
    "    loader = model.loader(batch_size=128, shuffle=True)\n",
    "    optimizer = torch.optim.SparseAdam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    model.train()\n",
    "    for _ in tqdm(range(num_iterations), desc=\"Training Node2Vec\"):\n",
    "        total_loss = 0\n",
    "        for pos_rw, neg_rw in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        # print(f'Loss: {total_loss / len(loader):.4f}')\n",
    "    \n",
    "    # Get embeddings\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model()\n",
    "    \n",
    "    return embeddings.cpu()\n",
    "\n",
    "\n",
    "def visualize_embeddings(embeddings, nodes_to_highlight=None, labels=None, method='tsne'):\n",
    "    \"\"\"Visualize node embeddings using t-SNE or UMAP\"\"\"\n",
    "    print(f\"\\nVisualizing embeddings using {method}...\")\n",
    "    \n",
    "    # If more than 10,000 nodes, sample\n",
    "    if embeddings.shape[0] > 10000:\n",
    "        sample_size = 10000\n",
    "        indices = torch.randperm(embeddings.shape[0])[:sample_size]\n",
    "        sample_embeddings = embeddings[indices]\n",
    "        \n",
    "        # Adjust highlight indices if provided\n",
    "        if nodes_to_highlight is not None:\n",
    "            # Keep only those in the sample\n",
    "            mask = torch.isin(indices, torch.tensor(nodes_to_highlight))\n",
    "            highlight_indices = torch.where(mask)[0]\n",
    "            if len(highlight_indices) == 0:\n",
    "                # Force include some highlights if none were sampled\n",
    "                sample_size = 9900\n",
    "                indices = torch.randperm(embeddings.shape[0])[:sample_size]\n",
    "                sample_indices = torch.cat([indices, torch.tensor(nodes_to_highlight[:100])])\n",
    "                sample_embeddings = embeddings[sample_indices]\n",
    "                highlight_indices = torch.arange(sample_size, sample_size + min(100, len(nodes_to_highlight)))\n",
    "        else:\n",
    "            highlight_indices = None\n",
    "            \n",
    "        # Adjust labels if provided\n",
    "        if labels is not None:\n",
    "            sample_labels = [labels[i.item()] for i in indices]\n",
    "        else:\n",
    "            sample_labels = None\n",
    "    else:\n",
    "        sample_embeddings = embeddings\n",
    "        highlight_indices = nodes_to_highlight\n",
    "        sample_labels = labels\n",
    "    \n",
    "    # Choose dimensionality reduction method\n",
    "    if method == 'tsne':\n",
    "        from sklearn.manifold import TSNE\n",
    "        reducer = TSNE(n_components=2, random_state=42)\n",
    "    elif method == 'umap':\n",
    "        try:\n",
    "            import umap\n",
    "            reducer = umap.UMAP(random_state=42)\n",
    "        except ImportError:\n",
    "            print(\"UMAP not installed, falling back to t-SNE\")\n",
    "            from sklearn.manifold import TSNE\n",
    "            reducer = TSNE(n_components=2, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown visualization method: {method}\")\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    reduced = reducer.fit_transform(sample_embeddings.numpy())\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot all points first\n",
    "    if sample_labels is not None:\n",
    "        unique_labels = sorted(set(sample_labels))\n",
    "        colors = plt.cm.tab20(np.linspace(0, 1, len(unique_labels)))\n",
    "        label_to_color = {label: color for label, color in zip(unique_labels, colors)}\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            mask = [l == label for l in sample_labels]\n",
    "            plt.scatter(reduced[mask, 0], reduced[mask, 1], s=5, alpha=0.5, \n",
    "                       color=label_to_color[label], label=f\"Community {label}\")\n",
    "        plt.legend(markerscale=2)\n",
    "    else:\n",
    "        plt.scatter(reduced[:, 0], reduced[:, 1], s=5, alpha=0.3, color='blue')\n",
    "    \n",
    "    # Highlight specific nodes if provided\n",
    "    if highlight_indices is not None:\n",
    "        plt.scatter(reduced[highlight_indices, 0], reduced[highlight_indices, 1], \n",
    "                   s=30, color='red', marker='x')\n",
    "    \n",
    "    plt.title(f\"Node Embeddings Visualization ({method.upper()})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'node_embeddings_{method}.png', dpi=300)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "835fdb8e-63b8-4c9c-8e98-ae2939186742",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 4. Path Analysis and Structural Properties\n",
    "\n",
    "def find_shortest_paths(data, source_nodes, target_nodes, max_nodes=1000):\n",
    "    \"\"\"Find shortest paths between source and target nodes\"\"\"\n",
    "    # For large graphs, we need to limit the search\n",
    "    if data.num_nodes > max_nodes:\n",
    "        # Use bidirectional search or approximation methods\n",
    "        print(\"Graph too large for exhaustive shortest path search\")\n",
    "        print(\"Computing approximate shortest paths...\")\n",
    "        \n",
    "        # Convert to scipy sparse matrix for efficient path computation\n",
    "        adj = to_scipy_sparse_matrix(data.edge_index, num_nodes=data.num_nodes)\n",
    "        \n",
    "        results = []\n",
    "        for source in source_nodes:\n",
    "            for target in target_nodes:\n",
    "                if source == target:\n",
    "                    results.append((source, target, 0, [source]))\n",
    "                    continue\n",
    "                    \n",
    "                # Compute shortest path length using scipy sparse\n",
    "                dist = sparse.csgraph.shortest_path(adj, directed=False, indices=[source])\n",
    "                distance = dist[0, target]\n",
    "                \n",
    "                if np.isfinite(distance):\n",
    "                    # We found a path, but we don't have the full path\n",
    "                    # For large graphs, we approximate\n",
    "                    results.append((source, target, int(distance), \n",
    "                                    [source, '...', target]))\n",
    "                else:\n",
    "                    results.append((source, target, float('inf'), []))\n",
    "        \n",
    "        return results\n",
    "    else:\n",
    "        # Use networkx for smaller graphs to get actual paths\n",
    "        G = nx.Graph()\n",
    "        edge_index = data.edge_index.numpy()\n",
    "        edges = list(zip(edge_index[0], edge_index[1]))\n",
    "        G.add_edges_from(edges)\n",
    "        \n",
    "        results = []\n",
    "        for source in source_nodes:\n",
    "            for target in target_nodes:\n",
    "                try:\n",
    "                    path = nx.shortest_path(G, source=source, target=target)\n",
    "                    results.append((source, target, len(path)-1, path))\n",
    "                except nx.NetworkXNoPath:\n",
    "                    results.append((source, target, float('inf'), []))\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "def compute_structural_properties(data):\n",
    "    \"\"\"Compute structural properties of the graph\"\"\"\n",
    "    print(\"\\nComputing structural properties...\")\n",
    "    \n",
    "    # Create edge weights tensor if not present\n",
    "    if not hasattr(data, 'edge_attr') or data.edge_attr is None:\n",
    "        data.edge_attr = torch.ones(data.edge_index.size(1))\n",
    "    \n",
    "    # Convert to scipy sparse matrix\n",
    "    adj = to_scipy_sparse_matrix(data.edge_index, edge_attr=data.edge_attr, num_nodes=data.num_nodes)\n",
    "    \n",
    "    # Compute average clustering coefficient for a sample of nodes\n",
    "    if data.num_nodes > 1000:\n",
    "        # Sample nodes for clustering coefficient calculation\n",
    "        sample_size = min(1000, data.num_nodes)\n",
    "        sample_nodes = np.random.choice(data.num_nodes, size=sample_size, replace=False)\n",
    "        \n",
    "        # Convert to networkx for clustering coefficient calculation\n",
    "        G = nx.Graph()\n",
    "        edge_index = data.edge_index.numpy()\n",
    "        edges = list(zip(edge_index[0], edge_index[1]))\n",
    "        G.add_edges_from(edges)\n",
    "        \n",
    "        clustering_coefficients = []\n",
    "        for node in tqdm(sample_nodes, desc=\"Computing clustering coefficients\"):\n",
    "            cc = nx.clustering(G, node)\n",
    "            clustering_coefficients.append(cc)\n",
    "        \n",
    "        avg_clustering = np.mean(clustering_coefficients)\n",
    "        print(f\"Average clustering coefficient (sampled): {avg_clustering:.4f}\")\n",
    "    else:\n",
    "        G = nx.Graph()\n",
    "        edge_index = data.edge_index.numpy()\n",
    "        edges = list(zip(edge_index[0], edge_index[1]))\n",
    "        G.add_edges_from(edges)\n",
    "        avg_clustering = nx.average_clustering(G)\n",
    "        print(f\"Average clustering coefficient: {avg_clustering:.4f}\")\n",
    "    \n",
    "    # Compute graph diameter (or estimate for large graphs)\n",
    "    if data.num_nodes > 1000:\n",
    "        print(\"Graph too large for exact diameter computation\")\n",
    "        print(\"Estimating effective diameter...\")\n",
    "        \n",
    "        # Sample some source nodes for BFS\n",
    "        sample_size = min(20, data.num_nodes)\n",
    "        sample_nodes = np.random.choice(data.num_nodes, size=sample_size, replace=False)\n",
    "        \n",
    "        max_distances = []\n",
    "        for source in tqdm(sample_nodes, desc=\"Estimating diameter\"):\n",
    "            # Run BFS from source\n",
    "            dist = sparse.csgraph.shortest_path(adj, directed=False, indices=[source])\n",
    "            # Get maximum finite distance\n",
    "            finite_distances = dist[np.isfinite(dist)]\n",
    "            if len(finite_distances) > 0:\n",
    "                max_distances.append(np.max(finite_distances))\n",
    "        \n",
    "        if max_distances:\n",
    "            estimated_diameter = np.max(max_distances)\n",
    "            print(f\"Estimated graph diameter: {estimated_diameter:.1f}\")\n",
    "        else:\n",
    "            print(\"Could not estimate diameter - graph may be disconnected\")\n",
    "    else:\n",
    "        try:\n",
    "            diameter = nx.diameter(G)\n",
    "            print(f\"Graph diameter: {diameter}\")\n",
    "        except nx.NetworkXError:\n",
    "            print(\"Graph is not connected, diameter undefined\")\n",
    "    \n",
    "    # Compute assortativity coefficient\n",
    "    if data.num_nodes <= 10000:\n",
    "        try:\n",
    "            degrees = dict(G.degree())\n",
    "            assortativity = nx.degree_assortativity_coefficient(G)\n",
    "            print(f\"Degree assortativity coefficient: {assortativity:.4f}\")\n",
    "            if assortativity > 0:\n",
    "                print(\"Positive assortativity: nodes tend to connect with similar-degree nodes\")\n",
    "            else:\n",
    "                print(\"Negative assortativity: nodes tend to connect with different-degree nodes\")\n",
    "        except:\n",
    "            print(\"Could not compute assortativity coefficient\")\n",
    "    \n",
    "    return {\n",
    "        'avg_clustering': avg_clustering,\n",
    "        'estimated_diameter': estimated_diameter if 'estimated_diameter' in locals() else None\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f049f1e-b46e-475b-8fa6-4759110390ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 5. User Similarity and Recommendation\n",
    "\n",
    "def compute_user_similarities(embeddings, user_ids, top_k=10):\n",
    "    \"\"\"Compute pairwise similarities between specified users based on embeddings\"\"\"\n",
    "    user_embeddings = embeddings[user_ids]\n",
    "    \n",
    "    # Normalize embeddings for cosine similarity\n",
    "    norm_embeddings = user_embeddings / user_embeddings.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute pairwise similarities\n",
    "    similarities = torch.mm(norm_embeddings, norm_embeddings.t())\n",
    "    \n",
    "    # Set self-similarities to 0 to find non-identical matches\n",
    "    similarities[torch.eye(len(user_ids), dtype=bool)] = 0\n",
    "    \n",
    "    # Get top_k similar users for each user\n",
    "    top_similarities, top_indices = similarities.topk(top_k, dim=1)\n",
    "    \n",
    "    return top_similarities, top_indices\n",
    "\n",
    "\n",
    "def find_similar_users(embeddings, query_user_idx, idx_to_user, top_k=10):\n",
    "    \"\"\"Find users similar to the query user based on embeddings\"\"\"\n",
    "    # Get query user embedding\n",
    "    query_embedding = embeddings[query_user_idx].unsqueeze(0)\n",
    "    \n",
    "    # Normalize for cosine similarity\n",
    "    query_norm = query_embedding / query_embedding.norm()\n",
    "    all_norm = embeddings / embeddings.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = torch.mm(query_norm, all_norm.t()).squeeze()\n",
    "    \n",
    "    # Set self-similarity to -1 to exclude from results\n",
    "    similarities[query_user_idx] = -1\n",
    "    \n",
    "    # Get top_k similar users\n",
    "    top_similarities, top_indices = similarities.topk(top_k)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nUsers most similar to {idx_to_user[query_user_idx]}:\")\n",
    "    for i, (idx, score) in enumerate(zip(top_indices, top_similarities)):\n",
    "        print(f\"{i+1}. {idx_to_user[idx.item()]} (similarity: {score.item():.4f})\")\n",
    "    \n",
    "    return top_indices.numpy(), top_similarities.numpy()\n",
    "\n",
    "\n",
    "def recommend_connections(data, query_user_idx, embeddings, idx_to_user, top_k=10):\n",
    "    \"\"\"Recommend new connections for a user based on embeddings and graph structure\"\"\"\n",
    "    # Get query user's current neighbors\n",
    "    row, col = data.edge_index\n",
    "    mask = (row == query_user_idx)\n",
    "    neighbors = col[mask].tolist()\n",
    "    \n",
    "    # Get node embeddings\n",
    "    query_embedding = embeddings[query_user_idx].unsqueeze(0)\n",
    "    \n",
    "    # Normalize for cosine similarity\n",
    "    query_norm = query_embedding / query_embedding.norm()\n",
    "    all_norm = embeddings / embeddings.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = torch.mm(query_norm, all_norm.t()).squeeze()\n",
    "    \n",
    "    # Set scores to -1 for the query user and existing neighbors\n",
    "    mask = torch.ones(data.num_nodes, dtype=torch.bool)\n",
    "    mask[query_user_idx] = False\n",
    "    mask[neighbors] = False\n",
    "    \n",
    "    # Only consider nodes that aren't already neighbors\n",
    "    similarities[~mask] = -1\n",
    "    \n",
    "    # Get top_k recommendations\n",
    "    top_similarities, top_indices = similarities.topk(top_k)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nRecommended connections for {idx_to_user[query_user_idx]}:\")\n",
    "    for i, (idx, score) in enumerate(zip(top_indices, top_similarities)):\n",
    "        print(f\"{i+1}. {idx_to_user[idx.item()]} (similarity: {score.item():.4f})\")\n",
    "    \n",
    "    return top_indices.numpy(), top_similarities.numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34aa2760-052b-4822-bb9b-31229ae3ed47",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 6. Main Analysis Function\n",
    "\n",
    "def analyze_large_reddit_graph(data, user_to_idx, idx_to_user, compute_embeddings=True):\n",
    "    \"\"\"Main function to analyze a large Reddit interaction graph\"\"\"\n",
    "    print(\"Starting analysis of large Reddit interaction graph...\")\n",
    "    \n",
    "    # 1. Basic graph analysis\n",
    "    degrees = compute_basic_stats(data)\n",
    "    top_users = find_influential_users(data, degrees, idx_to_user)\n",
    "    degree_counts = analyze_degree_distribution(degrees)\n",
    "    \n",
    "    # 2. Community detection\n",
    "    communities, top_communities = detect_communities(data, method=\"louvain\", max_nodes=100000)\n",
    "    analyze_top_communities(communities, top_communities, data, idx_to_user, degrees)\n",
    "    \n",
    "    # 3. Node embeddings (optional - can be computationally intensive)\n",
    "    if compute_embeddings:\n",
    "        embeddings = compute_node_embeddings(data, dim=128, num_iterations=5)\n",
    "        \n",
    "        # Create community labels for visualization\n",
    "        sample_size = min(10000, data.num_nodes)\n",
    "        sample_indices = torch.randperm(data.num_nodes)[:sample_size]\n",
    "        \n",
    "        # Map sampled nodes to communities\n",
    "        sample_community_labels = []\n",
    "        for idx in sample_indices:\n",
    "            idx_item = idx.item()\n",
    "            found = False\n",
    "            for comm_id, nodes in communities.items():\n",
    "                if idx_item in nodes:\n",
    "                    sample_community_labels.append(comm_id)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                sample_community_labels.append(-1)  # No community\n",
    "        \n",
    "        # Visualize embeddings with community colors\n",
    "        visualize_embeddings(embeddings, nodes_to_highlight=top_users, \n",
    "                            labels=sample_community_labels, method='tsne')\n",
    "        \n",
    "        # Find similar users to a few top users\n",
    "        for user_idx in top_users[:3]:\n",
    "            find_similar_users(embeddings, user_idx, idx_to_user, top_k=5)\n",
    "            recommend_connections(data, user_idx, embeddings, idx_to_user, top_k=5)\n",
    "    \n",
    "    # 4. Structural properties\n",
    "    structural_props = compute_structural_properties(data)\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "    return {\n",
    "        'degrees': degrees,\n",
    "        'communities': communities,\n",
    "        'structural_properties': structural_props,\n",
    "        'embeddings': embeddings if compute_embeddings else None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40e6bc85-0c9e-4eb2-923b-0c80b12609bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 532659\n",
      "Number of edges: 28462425 (undirected)\n",
      "Average degree: 106.87\n",
      "Max degree: 57911.0 (Node 461542: u/charavaka)\n",
      "Degree distribution stats: Min=0.0, Median=0.0, Mean=106.87\n",
      "25th percentile: 0.00\n",
      "50th percentile: 0.00\n",
      "75th percentile: 10.00\n",
      "90th percentile: 130.00\n",
      "95th percentile: 428.00\n",
      "99th percentile: 2244.00\n"
     ]
    }
   ],
   "source": [
    "degrees = compute_basic_stats(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1036adb1-6210-437f-8d24-d2c43ad695c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 users by connections:\n",
      "1. u/charavaka: 57911.0 connections\n",
      "2. u/svmk1987: 46140.0 connections\n",
      "3. u/whatsthebigdeal: 41992.0 connections\n",
      "4. u/siriusleesam: 40919.0 connections\n",
      "5. u/bhodrolok: 38602.0 connections\n",
      "6. u/ivarun: 36846.0 connections\n",
      "7. u/critfin: 36551.0 connections\n",
      "8. u/mrfreeze2000: 36454.0 connections\n",
      "9. u/uncertn_laaife: 35347.0 connections\n",
      "10. u/anthonygonsalvez: 34689.0 connections\n",
      "11. u/platinumgus18: 34394.0 connections\n",
      "12. u/froogler: 33118.0 connections\n",
      "13. u/-judeanpeoplesfront-: 32691.0 connections\n",
      "14. u/viksi: 32678.0 connections\n",
      "15. u/kash_if: 32123.0 connections\n",
      "16. u/moojo: 30623.0 connections\n",
      "17. u/tool_of_justice: 30442.0 connections\n",
      "18. u/wanderingmind: 30415.0 connections\n",
      "19. u/piezod: 29416.0 connections\n",
      "20. u/evereddy: 28858.0 connections\n"
     ]
    }
   ],
   "source": [
    "top_users = find_influential_users(data, degrees, idx_to_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93b98bfd-3f23-464b-9b4c-6e9728565a2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PERFORMING EXTREMELY SAFE DEGREE ANALYSIS ===\n",
      "\n",
      "Starting extremely safe degree analysis...\n",
      "Processed chunk 1/54\n",
      "Processed chunk 2/54\n",
      "Processed chunk 3/54\n",
      "Processed chunk 4/54\n",
      "Processed chunk 5/54\n",
      "Processed chunk 6/54\n",
      "Processed chunk 7/54\n",
      "Processed chunk 8/54\n",
      "Processed chunk 9/54\n",
      "Processed chunk 10/54\n",
      "Processed chunk 11/54\n",
      "Processed chunk 12/54\n",
      "Processed chunk 13/54\n",
      "Processed chunk 14/54\n",
      "Processed chunk 15/54\n",
      "Processed chunk 16/54\n",
      "Processed chunk 17/54\n",
      "Processed chunk 18/54\n",
      "Processed chunk 19/54\n",
      "Processed chunk 20/54\n",
      "Processed chunk 21/54\n",
      "Processed chunk 22/54\n",
      "Processed chunk 23/54\n",
      "Processed chunk 24/54\n",
      "Processed chunk 25/54\n",
      "Processed chunk 26/54\n",
      "Processed chunk 27/54\n",
      "Processed chunk 28/54\n",
      "Processed chunk 29/54\n",
      "Processed chunk 30/54\n",
      "Processed chunk 31/54\n",
      "Processed chunk 32/54\n",
      "Processed chunk 33/54\n",
      "Processed chunk 34/54\n",
      "Processed chunk 35/54\n",
      "Processed chunk 36/54\n",
      "Processed chunk 37/54\n",
      "Processed chunk 38/54\n",
      "Processed chunk 39/54\n",
      "Processed chunk 40/54\n",
      "Processed chunk 41/54\n",
      "Processed chunk 42/54\n",
      "Processed chunk 43/54\n",
      "Processed chunk 44/54\n",
      "Processed chunk 45/54\n",
      "Processed chunk 46/54\n",
      "Processed chunk 47/54\n",
      "Processed chunk 48/54\n",
      "Processed chunk 49/54\n",
      "Processed chunk 50/54\n",
      "Processed chunk 51/54\n",
      "Processed chunk 52/54\n",
      "Processed chunk 53/54\n",
      "Processed chunk 54/54\n",
      "Basic statistics calculation...\n",
      "Total nodes: 532659\n",
      "Min degree: 0.0\n",
      "Max degree: 57911.0\n",
      "Mean degree: 106.87\n",
      "Creating degree frequency table...\n",
      "Processed count chunk 1/54\n",
      "Processed count chunk 2/54\n",
      "Processed count chunk 3/54\n",
      "Processed count chunk 4/54\n",
      "Processed count chunk 5/54\n",
      "Processed count chunk 6/54\n",
      "Processed count chunk 7/54\n",
      "Processed count chunk 8/54\n",
      "Processed count chunk 9/54\n",
      "Processed count chunk 10/54\n",
      "Processed count chunk 11/54\n",
      "Processed count chunk 12/54\n",
      "Processed count chunk 13/54\n",
      "Processed count chunk 14/54\n",
      "Processed count chunk 15/54\n",
      "Processed count chunk 16/54\n",
      "Processed count chunk 17/54\n",
      "Processed count chunk 18/54\n",
      "Processed count chunk 19/54\n",
      "Processed count chunk 20/54\n",
      "Processed count chunk 21/54\n",
      "Processed count chunk 22/54\n",
      "Processed count chunk 23/54\n",
      "Processed count chunk 24/54\n",
      "Processed count chunk 25/54\n",
      "Processed count chunk 26/54\n",
      "Processed count chunk 27/54\n",
      "Processed count chunk 28/54\n",
      "Processed count chunk 29/54\n",
      "Processed count chunk 30/54\n",
      "Processed count chunk 31/54\n",
      "Processed count chunk 32/54\n",
      "Processed count chunk 33/54\n",
      "Processed count chunk 34/54\n",
      "Processed count chunk 35/54\n",
      "Processed count chunk 36/54\n",
      "Processed count chunk 37/54\n",
      "Processed count chunk 38/54\n",
      "Processed count chunk 39/54\n",
      "Processed count chunk 40/54\n",
      "Processed count chunk 41/54\n",
      "Processed count chunk 42/54\n",
      "Processed count chunk 43/54\n",
      "Processed count chunk 44/54\n",
      "Processed count chunk 45/54\n",
      "Processed count chunk 46/54\n",
      "Processed count chunk 47/54\n",
      "Processed count chunk 48/54\n",
      "Processed count chunk 49/54\n",
      "Processed count chunk 50/54\n",
      "Processed count chunk 51/54\n",
      "Processed count chunk 52/54\n",
      "Processed count chunk 53/54\n",
      "Processed count chunk 54/54\n",
      "Saving degree counts to file...\n",
      "Degree count data saved to degree_counts.txt\n",
      "You can use this file later to create visualizations without memory issues\n",
      "\n",
      "Degree analysis completed successfully\n",
      "Summary: {'nodes': 532659, 'min_degree': 0.0, 'max_degree': 57911.0, 'mean_degree': 106.86921651563195}\n"
     ]
    }
   ],
   "source": [
    "# Call the extremely safe function\n",
    "try:\n",
    "    print(\"\\n=== PERFORMING EXTREMELY SAFE DEGREE ANALYSIS ===\\n\")\n",
    "    basic_stats = extremely_safe_analyze_degrees(degrees)\n",
    "    print(f\"\\nDegree analysis completed successfully\")\n",
    "    print(f\"Summary: {basic_stats}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during analysis: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74edf717-7dfd-4c12-975c-3dee63cbb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch_geometric.utils import degree, k_hop_subgraph, to_scipy_sparse_matrix\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "def save_degrees_to_file(data, filename=\"degree_data.txt\"):\n",
    "    \"\"\"Save degree information to file in a memory-efficient way\"\"\"\n",
    "    print(\"Computing and saving node degrees...\")\n",
    "    row, col = data.edge_index\n",
    "    \n",
    "    # Process in chunks to avoid memory issues\n",
    "    chunk_size = 1000000  # Adjust based on available memory\n",
    "    total_nodes = data.num_nodes\n",
    "    \n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"NodeID,Degree\\n\")\n",
    "        \n",
    "        for start in range(0, total_nodes, chunk_size):\n",
    "            end = min(start + chunk_size, total_nodes)\n",
    "            print(f\"Processing nodes {start} to {end-1}...\")\n",
    "            \n",
    "            # Calculate degrees for this chunk of nodes\n",
    "            node_indices = torch.arange(start, end)\n",
    "            chunk_degrees = torch.zeros(end - start, dtype=torch.long)\n",
    "            \n",
    "            # Count edges for each node in this chunk\n",
    "            mask = (row >= start) & (row < end)\n",
    "            chunk_row = row[mask] - start\n",
    "            for idx in chunk_row:\n",
    "                chunk_degrees[idx] += 1\n",
    "            \n",
    "            # Write to file\n",
    "            for i, d in enumerate(chunk_degrees):\n",
    "                node_id = start + i\n",
    "                f.write(f\"{node_id},{d.item()}\\n\")\n",
    "            \n",
    "            # Force garbage collection\n",
    "            del chunk_degrees, mask, chunk_row\n",
    "            gc.collect()\n",
    "    \n",
    "    print(f\"Degree data saved to {filename}\")\n",
    "    return filename\n",
    "\n",
    "def create_degree_counts_file(degree_file=\"degree_data.txt\", output_file=\"degree_counts.txt\"):\n",
    "    \"\"\"Create a frequency count of degrees from the degree data file\"\"\"\n",
    "    print(\"Creating degree counts file...\")\n",
    "    \n",
    "    # Count frequencies from file to avoid memory issues\n",
    "    degree_counts = {}\n",
    "    \n",
    "    # Read the file in chunks\n",
    "    chunk_size = 100000  # Adjust based on available memory\n",
    "    chunk_iter = pd.read_csv(degree_file, chunksize=chunk_size)\n",
    "    \n",
    "    for chunk in chunk_iter:\n",
    "        degrees = chunk['Degree'].values\n",
    "        for d in degrees:\n",
    "            if d in degree_counts:\n",
    "                degree_counts[d] += 1\n",
    "            else:\n",
    "                degree_counts[d] = 1\n",
    "        \n",
    "        # Force garbage collection\n",
    "        del degrees\n",
    "        gc.collect()\n",
    "    \n",
    "    # Write counts to file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"Degree,Count\\n\")\n",
    "        for d in sorted(degree_counts.keys()):\n",
    "            f.write(f\"{d},{degree_counts[d]}\\n\")\n",
    "    \n",
    "    print(f\"Degree counts saved to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def plot_degree_ccdf(counts_file=\"degree_counts.txt\"):\n",
    "    \"\"\"\n",
    "    Plot the Complementary Cumulative Distribution Function (CCDF)\n",
    "    from the counts file, which is more memory efficient than using raw degrees\n",
    "    \"\"\"\n",
    "    print(\"Plotting CCDF...\")\n",
    "    # Load the degree counts from file\n",
    "    df = pd.read_csv(counts_file)\n",
    "    degrees = df['Degree'].values\n",
    "    counts = df['Count'].values\n",
    "    \n",
    "    # Calculate total number of nodes\n",
    "    total_nodes = np.sum(counts)\n",
    "    \n",
    "    # Calculate CCDF points\n",
    "    unique_degrees = np.sort(degrees)\n",
    "    ccdf_values = np.zeros_like(unique_degrees, dtype=float)\n",
    "    \n",
    "    # For each degree value, sum up all counts for degrees >= that value\n",
    "    for i, d in enumerate(unique_degrees):\n",
    "        ccdf_values[i] = np.sum(counts[degrees >= d]) / total_nodes\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.loglog(unique_degrees, ccdf_values, 'bo', markersize=2, alpha=0.5)\n",
    "    plt.xlabel('Degree (log scale)')\n",
    "    plt.ylabel('CCDF: P(X ≥ x) (log scale)')\n",
    "    plt.title('Degree CCDF (Complementary Cumulative Distribution Function)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('output_123/degree_ccdf.png', dpi=300)\n",
    "    plt.close()  # Close to free memory\n",
    "    print(\"CCDF plot saved to 'degree_ccdf.png'\")\n",
    "\n",
    "def find_top_nodes(degree_file=\"degree_data.txt\", top_n=20, user_mapping=None):\n",
    "    \"\"\"Find the top N nodes by degree in a memory-efficient way\"\"\"\n",
    "    print(f\"Finding top {top_n} nodes by degree...\")\n",
    "    \n",
    "    # Use pandas to read the file and find top nodes\n",
    "    # This is more memory efficient than loading all degrees into memory\n",
    "    top_df = pd.read_csv(degree_file).nlargest(top_n, 'Degree')\n",
    "    \n",
    "    print(f\"\\nTop {top_n} nodes by connections:\")\n",
    "    for i, (idx, row) in enumerate(top_df.iterrows()):\n",
    "        node_id = row['NodeID']\n",
    "        degree_val = row['Degree']\n",
    "        \n",
    "        if user_mapping and node_id in user_mapping:\n",
    "            print(f\"{i+1}. Node {node_id} ({user_mapping[node_id]}): {degree_val} connections\")\n",
    "        else:\n",
    "            print(f\"{i+1}. Node {node_id}: {degree_val} connections\")\n",
    "    \n",
    "    return top_df['NodeID'].values.tolist()\n",
    "\n",
    "def visualize_ego_network(data, center_node, hops=1, max_nodes=100):\n",
    "    \"\"\"\n",
    "    Visualize the ego network (local neighborhood) of a specific node\n",
    "    \"\"\"\n",
    "    print(f\"Visualizing ego network for node {center_node}...\")\n",
    "    # Extract k-hop subgraph\n",
    "    subset, edge_index, mapping, edge_mask = k_hop_subgraph(\n",
    "        node_idx=[center_node], \n",
    "        num_hops=hops,\n",
    "        edge_index=data.edge_index,\n",
    "        relabel_nodes=True,\n",
    "        num_nodes=data.num_nodes\n",
    "    )\n",
    "    \n",
    "    # If too large, sample nodes\n",
    "    if len(subset) > max_nodes:\n",
    "        print(f\"Ego network too large ({len(subset)} nodes). Sampling {max_nodes} nodes.\")\n",
    "        # Keep the center node and sample the rest\n",
    "        center_idx = mapping.tolist().index(center_node)\n",
    "        other_indices = list(range(len(subset)))\n",
    "        other_indices.remove(center_idx)\n",
    "        \n",
    "        # Randomly sample additional nodes\n",
    "        sampled_indices = np.random.choice(other_indices, min(max_nodes-1, len(other_indices)), replace=False).tolist()\n",
    "        sampled_indices.append(center_idx)  # Add center node back\n",
    "        \n",
    "        # Create a new mapping and edge index with only the sampled nodes\n",
    "        new_subset = subset[sampled_indices]\n",
    "        node_map = {old_idx: new_idx for new_idx, old_idx in enumerate(sampled_indices)}\n",
    "        \n",
    "        # Filter edges to only include sampled nodes\n",
    "        row, col = edge_index\n",
    "        edge_mask = torch.zeros(row.size(0), dtype=torch.bool)\n",
    "        for i, (src, dst) in enumerate(zip(row, col)):\n",
    "            if src.item() in node_map and dst.item() in node_map:\n",
    "                edge_mask[i] = True\n",
    "        \n",
    "        row = row[edge_mask]\n",
    "        col = col[edge_mask]\n",
    "        new_row = torch.tensor([node_map[idx.item()] for idx in row])\n",
    "        new_col = torch.tensor([node_map[idx.item()] for idx in col])\n",
    "        new_edge_index = torch.stack([new_row, new_col])\n",
    "        \n",
    "        subset = new_subset\n",
    "        edge_index = new_edge_index\n",
    "    \n",
    "    # Calculate node sizes based on degree\n",
    "    row, col = edge_index\n",
    "    degrees = degree(row, num_nodes=len(subset))\n",
    "    node_sizes = 10 + 50 * (degrees / max(degrees.max(), 1))\n",
    "    \n",
    "    # Create a simple plot using matplotlib\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Extract node positions using a force-directed layout (this is lightweight)\n",
    "    pos = {}\n",
    "    # Convert to NetworkX just for layout calculation (this is the most efficient approach)\n",
    "    G = nx.Graph()\n",
    "    for i in range(len(subset)):\n",
    "        G.add_node(i)\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        G.add_edge(edge_index[0, i].item(), edge_index[1, i].item())\n",
    "    \n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "    # Draw the network\n",
    "    for i, j in zip(edge_index[0], edge_index[1]):\n",
    "        i, j = i.item(), j.item()\n",
    "        plt.plot([pos[i][0], pos[j][0]], [pos[i][1], pos[j][1]], 'k-', alpha=0.1, linewidth=0.5)\n",
    "    \n",
    "    # Draw the center node with a different color\n",
    "    center_idx = mapping.tolist().index(center_node)\n",
    "    for i in range(len(subset)):\n",
    "        if i == center_idx:\n",
    "            plt.scatter(pos[i][0], pos[i][1], s=node_sizes[i].item()*2, c='red', alpha=0.8)\n",
    "        else:\n",
    "            plt.scatter(pos[i][0], pos[i][1], s=node_sizes[i].item(), c='blue', alpha=0.5)\n",
    "    \n",
    "    plt.title(f\"{hops}-hop Ego Network of Node {center_node}\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'ego_network_node_{center_node}.png', dpi=300)\n",
    "    plt.close()  # Close to free memory\n",
    "    print(f\"Ego network visualization saved to 'ego_network_node_{center_node}.png'\")\n",
    "    \n",
    "    # Return some statistics about the ego network\n",
    "    return {\n",
    "        \"num_nodes\": len(subset),\n",
    "        \"num_edges\": edge_index.shape[1] // 2,\n",
    "        \"avg_degree\": degrees.float().mean().item()\n",
    "    }\n",
    "\n",
    "def visualize_random_subgraph(data, num_nodes=500, seed=42):\n",
    "    \"\"\"\n",
    "    Sample a random subgraph for visualization\n",
    "    \"\"\"\n",
    "    print(f\"Visualizing random subgraph with {num_nodes} nodes...\")\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Sample random nodes\n",
    "    sampled_nodes = np.random.choice(data.num_nodes, size=min(num_nodes, data.num_nodes), replace=False)\n",
    "    sampled_nodes = torch.tensor(sampled_nodes)\n",
    "    \n",
    "    # Extract the subgraph\n",
    "    row, col = data.edge_index\n",
    "    mask = torch.isin(row, sampled_nodes) & torch.isin(col, sampled_nodes)\n",
    "    sub_edge_index = data.edge_index[:, mask]\n",
    "    \n",
    "    # Create node mapping\n",
    "    node_map = {int(node): i for i, node in enumerate(sampled_nodes)}\n",
    "    \n",
    "    # Relabel nodes\n",
    "    sub_row = torch.tensor([node_map[int(node)] for node in sub_edge_index[0] if int(node) in node_map])\n",
    "    sub_col = torch.tensor([node_map[int(node)] for node in sub_edge_index[1] if int(node) in node_map])\n",
    "    \n",
    "    if len(sub_row) == 0:\n",
    "        print(\"No edges found in the sampled subgraph. Try increasing the sample size.\")\n",
    "        return\n",
    "        \n",
    "    sub_edge_index = torch.stack([sub_row, sub_col])\n",
    "    \n",
    "    # Calculate node degrees\n",
    "    sub_degrees = degree(sub_edge_index[0], num_nodes=len(sampled_nodes))\n",
    "    \n",
    "    # Create a simple NetworkX graph for layout\n",
    "    G = nx.Graph()\n",
    "    for i in range(len(sampled_nodes)):\n",
    "        G.add_node(i)\n",
    "    for i in range(sub_edge_index.shape[1]):\n",
    "        G.add_edge(sub_edge_index[0, i].item(), sub_edge_index[1, i].item())\n",
    "    \n",
    "    # Calculate layout\n",
    "    pos = nx.spring_layout(G, seed=seed)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    # Plot edges\n",
    "    for i, j in zip(sub_edge_index[0], sub_edge_index[1]):\n",
    "        i, j = i.item(), j.item()\n",
    "        plt.plot([pos[i][0], pos[j][0]], [pos[i][1], pos[j][1]], 'k-', alpha=0.2, linewidth=0.5)\n",
    "    \n",
    "    # Plot nodes\n",
    "    node_sizes = 10 + 50 * (sub_degrees / max(sub_degrees.max(), 1))\n",
    "    for i in range(len(sampled_nodes)):\n",
    "        if i in pos:  # Check if node is in the position dictionary\n",
    "            plt.scatter(pos[i][0], pos[i][1], s=node_sizes[i].item(), \n",
    "                        c=plt.cm.viridis(float(sub_degrees[i])/max(sub_degrees.max().item(), 1)), \n",
    "                        alpha=0.7)\n",
    "    \n",
    "    plt.title(f'Random Subgraph with {len(sampled_nodes)} Nodes')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output_123/random_subgraph.png', dpi=300)\n",
    "    plt.close()  # Close to free memory\n",
    "    print(\"Random subgraph visualization saved to 'random_subgraph.png'\")\n",
    "\n",
    "def analyze_connected_components(data, max_components_to_display=10, sample_fraction=None):\n",
    "    \"\"\"\n",
    "    Analyze connected components of the graph\n",
    "    \n",
    "    Args:\n",
    "        data: PyG data object\n",
    "        max_components_to_display: Number of largest components to display\n",
    "        sample_fraction: If not None, analyze a random sample of the graph to save memory\n",
    "    \"\"\"\n",
    "    print(\"Analyzing connected components...\")\n",
    "    \n",
    "    if sample_fraction is not None and sample_fraction < 1.0:\n",
    "        # Sample a fraction of nodes to reduce memory usage\n",
    "        num_sample_nodes = int(data.num_nodes * sample_fraction)\n",
    "        print(f\"Sampling {num_sample_nodes} nodes ({sample_fraction*100:.1f}% of graph) for component analysis...\")\n",
    "        \n",
    "        # Sample random nodes\n",
    "        np.random.seed(42)\n",
    "        sampled_nodes = np.random.choice(data.num_nodes, size=num_sample_nodes, replace=False)\n",
    "        sampled_nodes = torch.tensor(sampled_nodes)\n",
    "        \n",
    "        # Extract the subgraph\n",
    "        row, col = data.edge_index\n",
    "        mask = torch.isin(row, sampled_nodes) & torch.isin(col, sampled_nodes)\n",
    "        sub_edge_index = data.edge_index[:, mask]\n",
    "        \n",
    "        # Create node mapping\n",
    "        node_map = {int(node): i for i, node in enumerate(sampled_nodes)}\n",
    "        \n",
    "        # Relabel nodes\n",
    "        sub_row = torch.tensor([node_map[int(node)] for node in sub_edge_index[0] if int(node) in node_map])\n",
    "        sub_col = torch.tensor([node_map[int(node)] for node in sub_edge_index[1] if int(node) in node_map])\n",
    "        \n",
    "        if len(sub_row) == 0:\n",
    "            print(\"No edges found in the sampled subgraph. Try increasing the sample size.\")\n",
    "            return\n",
    "        \n",
    "        sub_edge_index = torch.stack([sub_row, sub_col])\n",
    "        \n",
    "        # Convert to sparse matrix\n",
    "        adj_sparse = to_scipy_sparse_matrix(sub_edge_index, num_nodes=num_sample_nodes)\n",
    "        total_nodes = num_sample_nodes\n",
    "        \n",
    "    else:\n",
    "        # Use the full graph\n",
    "        # Convert to sparse matrix for efficient component calculation\n",
    "        adj_sparse = to_scipy_sparse_matrix(data.edge_index, num_nodes=data.num_nodes)\n",
    "        total_nodes = data.num_nodes\n",
    "    \n",
    "    print(\"Computing connected components...\")\n",
    "    try:\n",
    "        n_components, labels = sparse.csgraph.connected_components(\n",
    "            adj_sparse, directed=False, return_labels=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Total number of connected components: {n_components}\")\n",
    "        \n",
    "        # Count component sizes\n",
    "        component_counts = {}\n",
    "        for label in labels:\n",
    "            if label in component_counts:\n",
    "                component_counts[label] += 1\n",
    "            else:\n",
    "                component_counts[label] = 1\n",
    "        \n",
    "        # Sort by size (descending)\n",
    "        sorted_components = sorted(component_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Plot the top N component sizes\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        top_components = sorted_components[:max_components_to_display]\n",
    "        plt.bar(range(len(top_components)), [size for _, size in top_components])\n",
    "        plt.xlabel('Component ID')\n",
    "        plt.ylabel('Component Size (# nodes)')\n",
    "        plt.title(f'Top {len(top_components)} Connected Component Sizes')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('output_123/component_sizes.png', dpi=300)\n",
    "        plt.close()  # Close to free memory\n",
    "        \n",
    "        # Print component statistics\n",
    "        largest_component_size = sorted_components[0][1]\n",
    "        largest_component_percentage = (largest_component_size / total_nodes) * 100\n",
    "        \n",
    "        print(f\"Largest component has {largest_component_size} nodes \" \n",
    "              f\"({largest_component_percentage:.2f}% of the analyzed graph)\")\n",
    "        print(f\"Number of singleton nodes: {sum(1 for _, size in sorted_components if size == 1)}\")\n",
    "        \n",
    "        # Create a new figure for component size distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sizes = [size for _, size in sorted_components]\n",
    "        plt.loglog(range(1, len(sizes) + 1), sizes, 'o-', markersize=3)\n",
    "        plt.xlabel('Component Rank (log scale)')\n",
    "        plt.ylabel('Component Size (log scale)')\n",
    "        plt.title('Component Size Distribution')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig('output_123/component_size_distribution.png', dpi=300)\n",
    "        plt.close()  # Close to free memory\n",
    "        print(\"Component visualizations saved to 'component_sizes.png' and 'component_size_distribution.png'\")\n",
    "        \n",
    "        if sample_fraction is not None:\n",
    "            print(f\"Note: Component analysis was performed on a {sample_fraction*100:.1f}% sample of the graph\")\n",
    "        \n",
    "        return sorted_components\n",
    "        \n",
    "    except MemoryError:\n",
    "        print(\"Memory error when calculating connected components.\")\n",
    "        print(\"Try using a smaller sample_fraction parameter (e.g., 0.1) to analyze a subset of the graph.\")\n",
    "        return None\n",
    "\n",
    "def stepwise_analysis(data, user_mapping=None):\n",
    "    \"\"\"Run analysis steps one by one with memory cleanup between each step\"\"\"\n",
    "    # Step 1: Save degrees to file\n",
    "    degree_file = save_degrees_to_file(data)\n",
    "    gc.collect()\n",
    "    \n",
    "    # Step 2: Create degree counts file\n",
    "    counts_file = create_degree_counts_file(degree_file)\n",
    "    gc.collect()\n",
    "    \n",
    "    # Step 3: Plot degree distribution\n",
    "    plot_degree_distribution(counts_file, log_scale=True)\n",
    "    gc.collect()\n",
    "    \n",
    "    # Step 4: Plot CCDF\n",
    "    plot_degree_ccdf(counts_file)\n",
    "    gc.collect()\n",
    "    \n",
    "    # Step 5: Find top nodes\n",
    "    top_nodes = find_top_nodes(degree_file, top_n=20, user_mapping=user_mapping)\n",
    "    gc.collect()\n",
    "    \n",
    "    # Step 6: Visualize ego network of most connected node\n",
    "    if top_nodes:\n",
    "        try:\n",
    "            visualize_ego_network(data, top_nodes[0], hops=1, max_nodes=100)\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"Error visualizing ego network: {str(e)}\")\n",
    "    \n",
    "    # Step 7: Visualize random subgraph\n",
    "    try:\n",
    "        visualize_random_subgraph(data, num_nodes=200)\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing random subgraph: {str(e)}\")\n",
    "        # Try with smaller sample\n",
    "        try:\n",
    "            visualize_random_subgraph(data, num_nodes=50)\n",
    "            gc.collect()\n",
    "        except:\n",
    "            print(\"Failed to visualize even a smaller random subgraph\")\n",
    "    \n",
    "    # Step 8: Analyze connected components (with sampling if needed)\n",
    "    try:\n",
    "        analyze_connected_components(data, sample_fraction=0.1)\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing connected components: {str(e)}\")\n",
    "        print(\"Try running component analysis separately with smaller sample\")\n",
    "    \n",
    "    print(\"Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7abb4269-13a2-4973-aee4-b907865ceaa0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import gc  # Garbage collector\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "def plot_degree_distribution_safe(counts_file=\"degree_counts.txt\", log_scale=True, max_display=None):\n",
    "    \"\"\"\n",
    "    Ultra-safe version of the degree distribution plotting function that uses\n",
    "    minimal memory and has multiple safeguards against kernel crashes.\n",
    "    \"\"\"\n",
    "    print(f\"\\nPlotting degree distribution from {counts_file} (ultra-safe mode)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step 1: Check if file exists\n",
    "    if not os.path.exists(counts_file):\n",
    "        print(f\"Error: File {counts_file} not found\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Load data without plotting first\n",
    "    try:\n",
    "        print(\"Reading data file...\")\n",
    "        df = pd.read_csv(counts_file)\n",
    "        \n",
    "        if 'Degree' not in df.columns or 'Count' not in df.columns:\n",
    "            print(f\"Error: Required columns 'Degree' and 'Count' not found in {counts_file}\")\n",
    "            return\n",
    "        \n",
    "        # Apply filtering\n",
    "        if max_display:\n",
    "            print(f\"Limiting to degrees <= {max_display}\")\n",
    "            df = df[df['Degree'] <= max_display]\n",
    "        \n",
    "        # Convert to simple lists for minimal memory usage\n",
    "        degrees = df['Degree'].tolist()\n",
    "        counts = df['Count'].tolist()\n",
    "        \n",
    "        # Free memory\n",
    "        del df\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Data loaded: {len(degrees)} data points\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Save the data as a simple text file first (as backup)\n",
    "    try:\n",
    "        os.makedirs('output_123', exist_ok=True)\n",
    "        backup_file = 'output_123/degree_data_backup.txt'\n",
    "        print(f\"Saving backup data to {backup_file}...\")\n",
    "        \n",
    "        with open(backup_file, 'w') as f:\n",
    "            f.write(\"Degree,Count\\n\")\n",
    "            for d, c in zip(degrees, counts):\n",
    "                f.write(f\"{d},{c}\\n\")\n",
    "        \n",
    "        print(\"Backup saved successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not save backup data: {str(e)}\")\n",
    "    \n",
    "    # Step 4: Try to import matplotlib with a timeout\n",
    "    try:\n",
    "        print(\"Importing matplotlib...\")\n",
    "        \n",
    "        # Try to use Agg backend which is more stable\n",
    "        import matplotlib\n",
    "        matplotlib.use('Agg')  # Use non-interactive backend\n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "        print(\"Matplotlib imported successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error importing matplotlib: {str(e)}\")\n",
    "        print(\"Cannot generate plot. Data has been saved to backup file.\")\n",
    "        return\n",
    "    \n",
    "    # Step 5: Generate the plot with extensive error handling\n",
    "    try:\n",
    "        print(\"Preparing to generate plot...\")\n",
    "        \n",
    "        # Turn off interactive mode\n",
    "        plt.ioff()\n",
    "        \n",
    "        # Suppress warnings\n",
    "        warnings.filterwarnings('ignore')\n",
    "        \n",
    "        # Create figure with minimal memory footprint\n",
    "        plt.figure(figsize=(10, 6), dpi=100)\n",
    "        \n",
    "        if log_scale:\n",
    "            print(\"Preparing log-log plot...\")\n",
    "            # Filter out zeros and negative values for log scale\n",
    "            valid_points = [(d, c) for d, c in zip(degrees, counts) if d > 0 and c > 0]\n",
    "            \n",
    "            if not valid_points:\n",
    "                print(\"Error: No valid data points for log-log plot\")\n",
    "                return\n",
    "            \n",
    "            plot_degrees, plot_counts = zip(*valid_points)\n",
    "            \n",
    "            # Use scatter instead of plot for potentially better memory usage\n",
    "            plt.loglog(plot_degrees, plot_counts, 'o', markersize=2, alpha=0.5)\n",
    "            plt.xlabel('Degree (log scale)')\n",
    "            plt.ylabel('Count (log scale)')\n",
    "            plt.title('Degree Distribution (Log-Log Scale)')\n",
    "        else:\n",
    "            print(\"Preparing linear plot...\")\n",
    "            plt.scatter(degrees, counts, s=2, alpha=0.5)\n",
    "            plt.xlabel('Degree')\n",
    "            plt.ylabel('Count')\n",
    "            plt.title('Degree Distribution')\n",
    "        \n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Save figure with minimal DPI first as a test\n",
    "        test_file = 'output_123/degree_distribution_test.png'\n",
    "        print(f\"Saving test plot to {test_file}...\")\n",
    "        plt.savefig(test_file, dpi=72)\n",
    "        print(\"Test plot saved successfully\")\n",
    "        \n",
    "        # Now try the full resolution plot\n",
    "        save_path = 'output_123/degree_distribution.png'\n",
    "        print(f\"Saving final plot to {save_path}...\")\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        print(\"Final plot saved successfully\")\n",
    "        \n",
    "        # Close plot to free memory\n",
    "        plt.close('all')\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Degree distribution plot saved to '{save_path}'\")\n",
    "        print(f\"Total plotting time: {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during plotting: {str(e)}\")\n",
    "        \n",
    "        # Try to save a simplified plot as a last resort\n",
    "        try:\n",
    "            print(\"Attempting to save a simplified plot...\")\n",
    "            plt.figure(figsize=(8, 5), dpi=72)\n",
    "            \n",
    "            # Use even simpler plotting\n",
    "            if log_scale:\n",
    "                filtered_degrees = []\n",
    "                filtered_counts = []\n",
    "                for d, c in zip(degrees, counts):\n",
    "                    if d > 0 and c > 0:\n",
    "                        filtered_degrees.append(d)\n",
    "                        filtered_counts.append(c)\n",
    "                        \n",
    "                # Sample points if there are too many\n",
    "                if len(filtered_degrees) > 1000:\n",
    "                    indices = np.random.choice(len(filtered_degrees), 1000, replace=False)\n",
    "                    filtered_degrees = [filtered_degrees[i] for i in indices]\n",
    "                    filtered_counts = [filtered_counts[i] for i in indices]\n",
    "                \n",
    "                plt.loglog(filtered_degrees, filtered_counts, 'o', markersize=2)\n",
    "            else:\n",
    "                # Sample points if there are too many\n",
    "                if len(degrees) > 1000:\n",
    "                    indices = np.random.choice(len(degrees), 1000, replace=False)\n",
    "                    sampled_degrees = [degrees[i] for i in indices]\n",
    "                    sampled_counts = [counts[i] for i in indices]\n",
    "                    plt.scatter(sampled_degrees, sampled_counts, s=2)\n",
    "                else:\n",
    "                    plt.scatter(degrees, counts, s=2)\n",
    "            \n",
    "            plt.grid(True)\n",
    "            simple_path = 'output_123/degree_distribution_simple.png'\n",
    "            plt.savefig(simple_path, dpi=72)\n",
    "            plt.close('all')\n",
    "            print(f\"Simplified plot saved to {simple_path}\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"Failed to save simplified plot: {str(e2)}\")\n",
    "            print(\"Plot generation failed, but data was saved to backup file.\")\n",
    "\n",
    "# Alternative approach: Use plain text output if plotting is problematic\n",
    "def text_based_degree_distribution(counts_file=\"degree_counts.txt\", max_display=None, num_bins=10):\n",
    "    \"\"\"Generate a text-based visualization of the degree distribution\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nGenerating text-based degree distribution from {counts_file}...\")\n",
    "        \n",
    "        # Load data\n",
    "        df = pd.read_csv(counts_file)\n",
    "        \n",
    "        if max_display:\n",
    "            df = df[df['Degree'] <= max_display]\n",
    "        \n",
    "        degrees = df['Degree'].values\n",
    "        counts = df['Count'].values\n",
    "        \n",
    "        # Get summary statistics\n",
    "        max_degree = max(degrees)\n",
    "        total_nodes = sum(counts)\n",
    "        avg_degree = sum(degrees * counts) / total_nodes if total_nodes > 0 else 0\n",
    "        \n",
    "        print(\"\\n=== DEGREE DISTRIBUTION SUMMARY ===\")\n",
    "        print(f\"Total nodes: {total_nodes}\")\n",
    "        print(f\"Maximum degree: {max_degree}\")\n",
    "        print(f\"Average degree: {avg_degree:.2f}\")\n",
    "        \n",
    "        # Create bins for text histogram\n",
    "        if len(degrees) > num_bins:\n",
    "            # Create logarithmic bins\n",
    "            if max_degree > 100:\n",
    "                bins = np.logspace(0, np.log10(max_degree), num_bins+1)\n",
    "            else:\n",
    "                bins = np.linspace(0, max_degree, num_bins+1)\n",
    "            \n",
    "            bin_counts = np.zeros(num_bins)\n",
    "            \n",
    "            for d, c in zip(degrees, counts):\n",
    "                for i in range(num_bins):\n",
    "                    if bins[i] <= d < bins[i+1]:\n",
    "                        bin_counts[i] += c\n",
    "                        break\n",
    "            \n",
    "            # Print text histogram\n",
    "            print(\"\\n=== TEXT HISTOGRAM ===\")\n",
    "            max_bar_width = 50\n",
    "            max_count = max(bin_counts)\n",
    "            \n",
    "            for i in range(num_bins):\n",
    "                if bins[i+1] - bins[i] < 1:\n",
    "                    range_str = f\"{bins[i]:.1f}\"\n",
    "                else:\n",
    "                    range_str = f\"{int(bins[i])}-{int(bins[i+1])}\"\n",
    "                \n",
    "                bar_width = int((bin_counts[i] / max_count) * max_bar_width) if max_count > 0 else 0\n",
    "                bar = \"#\" * bar_width\n",
    "                \n",
    "                print(f\"Degree {range_str:10}: {bar} ({bin_counts[i]:.0f})\")\n",
    "        \n",
    "        # Save summary to file\n",
    "        with open('output_123/degree_distribution_summary.txt', 'w') as f:\n",
    "            f.write(\"=== DEGREE DISTRIBUTION SUMMARY ===\\n\")\n",
    "            f.write(f\"Total nodes: {total_nodes}\\n\")\n",
    "            f.write(f\"Maximum degree: {max_degree}\\n\")\n",
    "            f.write(f\"Average degree: {avg_degree:.2f}\\n\")\n",
    "            \n",
    "        print(\"\\nSummary saved to 'output_123/degree_distribution_summary.txt'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating text-based distribution: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf17f982-1330-44c3-a170-e6a2405debd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Plotting degree distribution from degree_counts.txt (ultra-safe mode)...\n",
      "Reading data file...\n",
      "Limiting to degrees <= 1000\n",
      "Data loaded: 1001 data points\n",
      "Saving backup data to output_123/degree_data_backup.txt...\n",
      "Backup saved successfully\n",
      "Importing matplotlib...\n",
      "Matplotlib imported successfully\n",
      "Preparing to generate plot...\n",
      "Preparing log-log plot...\n",
      "Saving test plot to output_123/degree_distribution_test.png...\n",
      "Test plot saved successfully\n",
      "Saving final plot to output_123/degree_distribution.png...\n",
      "Final plot saved successfully\n",
      "Degree distribution plot saved to 'output_123/degree_distribution.png'\n",
      "Total plotting time: 1.80 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import gc  # For garbage collection\n",
    "\n",
    "# Run the full analysis pipeline\n",
    "plot_degree_distribution_safe(\"degree_counts.txt\", log_scale=True, max_display=1000)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6975f770-b1b3-4bb6-a851-a428b0774404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting CCDF...\n",
      "CCDF plot saved to 'degree_ccdf.png'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23527"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_degree_ccdf(\"degree_counts.txt\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16230262-e7c8-4531-a4b2-ecf12e6170dd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.utils import degree\n",
    "from collections import Counter\n",
    "\n",
    "def save_degrees_to_file(data, filename=\"degree_data.txt\", idx_to_user=None):\n",
    "    \"\"\"Save degree information to file in a memory-efficient and FAST way\"\"\"\n",
    "    print(\"Computing and saving node degrees...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use PyG's optimized degree calculation instead of manual counting\n",
    "    print(\"Calculating degrees using optimized method...\")\n",
    "    deg_start = time.time()\n",
    "    \n",
    "    # This is MUCH faster than counting manually\n",
    "    degrees = degree(data.edge_index[0], num_nodes=data.num_nodes)\n",
    "    \n",
    "    print(f\"Degree calculation completed in {time.time() - deg_start:.2f} seconds\")\n",
    "    \n",
    "    # Saving to file\n",
    "    print(f\"Saving degrees to {filename}...\")\n",
    "    save_start = time.time()\n",
    "    \n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"NodeID,Degree\" + (\",Username\" if idx_to_user else \"\") + \"\\n\")\n",
    "        \n",
    "        # Process in chunks to avoid memory issues\n",
    "        chunk_size = 1000000  # Still chunk for very large graphs\n",
    "        total_nodes = data.num_nodes\n",
    "        \n",
    "        for start in range(0, total_nodes, chunk_size):\n",
    "            batch_start = time.time()\n",
    "            end = min(start + chunk_size, total_nodes)\n",
    "            \n",
    "            # Write this chunk to file\n",
    "            for node_id in range(start, end):\n",
    "                degree_val = degrees[node_id].item()\n",
    "                \n",
    "                if idx_to_user:\n",
    "                    username = idx_to_user.get(node_id, \"\")\n",
    "                    f.write(f\"{node_id},{degree_val},{username}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"{node_id},{degree_val}\\n\")\n",
    "            \n",
    "            if end - start >= 100000:  # Only log for large chunks\n",
    "                print(f\"Processed nodes {start:,} to {end-1:,} in {time.time() - batch_start:.2f} seconds\")\n",
    "    \n",
    "    # Also compute and save degree distribution statistics\n",
    "    print(\"Computing degree distribution statistics...\")\n",
    "    stats_start = time.time()\n",
    "    \n",
    "    # Count frequency of each degree\n",
    "    degree_counts = Counter(degrees.tolist())\n",
    "    \n",
    "    # Save degree distribution\n",
    "    dist_file = filename.replace(\".txt\", \"_counts.txt\")\n",
    "    with open(dist_file, \"w\") as f:\n",
    "        f.write(\"Degree,Count\\n\")\n",
    "        for deg, count in sorted(degree_counts.items()):\n",
    "            f.write(f\"{deg},{count}\\n\")\n",
    "    \n",
    "    print(f\"Degree distribution statistics saved to {dist_file}\")\n",
    "    print(f\"Statistics computation completed in {time.time() - stats_start:.2f} seconds\")\n",
    "    \n",
    "    # Print some statistics\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average degree: {degrees.float().mean().item():.2f}\")\n",
    "    print(f\"Maximum degree: {degrees.max().item()}\")\n",
    "    print(f\"Total nodes: {len(degrees):,}\")\n",
    "    \n",
    "    return filename, dist_file\n",
    "\n",
    "# Even faster version - if you only need the degree distribution\n",
    "def save_degree_distribution_only(data, filename=\"degree_counts.txt\"):\n",
    "    \"\"\"\n",
    "    Calculate and save only the degree distribution (faster than saving all degrees)\n",
    "    \"\"\"\n",
    "    print(\"Computing degree distribution (fast method)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Calculate degrees\n",
    "    print(\"Calculating degrees...\")\n",
    "    degrees = degree(data.edge_index[0], num_nodes=data.num_nodes)\n",
    "    print(f\"Degree calculation completed in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Compute distribution directly\n",
    "    print(\"Computing distribution...\")\n",
    "    dist_start = time.time()\n",
    "    \n",
    "    # Count frequency of each degree\n",
    "    degree_counts = {}\n",
    "    for d in degrees:\n",
    "        d_val = d.item()\n",
    "        if d_val in degree_counts:\n",
    "            degree_counts[d_val] += 1\n",
    "        else:\n",
    "            degree_counts[d_val] = 1\n",
    "    \n",
    "    # Save degree distribution\n",
    "    print(f\"Saving distribution to {filename}...\")\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"Degree,Count\\n\")\n",
    "        for deg, count in sorted(degree_counts.items()):\n",
    "            f.write(f\"{deg},{count}\\n\")\n",
    "    \n",
    "    print(f\"Degree distribution saved to {filename}\")\n",
    "    print(f\"Distribution computation completed in {time.time() - dist_start:.2f} seconds\")\n",
    "    print(f\"Total processing time: {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Print some statistics\n",
    "    avg_degree = sum(d * c for d, c in degree_counts.items()) / sum(degree_counts.values())\n",
    "    max_degree = max(degree_counts.keys())\n",
    "    print(f\"Average degree: {avg_degree:.2f}\")\n",
    "    print(f\"Maximum degree: {max_degree}\")\n",
    "    print(f\"Total nodes: {sum(degree_counts.values()):,}\")\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1bf9e28-a9da-4242-a044-2e6061053058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing and saving node degrees...\n",
      "Calculating degrees using optimized method...\n",
      "Degree calculation completed in 0.15 seconds\n",
      "Saving degrees to degree_data.txt...\n",
      "Processed nodes 0 to 532,658 in 3.29 seconds\n",
      "Computing degree distribution statistics...\n",
      "Degree distribution statistics saved to degree_data_counts.txt\n",
      "Statistics computation completed in 0.11 seconds\n",
      "Total processing time: 4.18 seconds\n",
      "Average degree: 106.87\n",
      "Maximum degree: 57911.0\n",
      "Total nodes: 532,659\n"
     ]
    }
   ],
   "source": [
    "# Full version - saves both node degrees and distribution\n",
    "node_degrees_file, distribution_file = save_degrees_to_file(data, filename=\"degree_data.txt\")\n",
    "\n",
    "# Fast version - only saves degree distribution\n",
    "# distribution_file = save_degree_distribution_only(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cc4c556-d7aa-48ac-b95c-196c21ea40fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding top 20 nodes by degree...\n",
      "\n",
      "Top 20 nodes by connections:\n",
      "1. Node 461542.0: 57911.0 connections\n",
      "2. Node 479353.0: 46140.0 connections\n",
      "3. Node 313875.0: 41992.0 connections\n",
      "4. Node 356599.0: 40919.0 connections\n",
      "5. Node 223927.0: 38602.0 connections\n",
      "6. Node 228186.0: 36846.0 connections\n",
      "7. Node 258305.0: 36551.0 connections\n",
      "8. Node 38569.0: 36454.0 connections\n",
      "9. Node 60011.0: 35347.0 connections\n",
      "10. Node 177072.0: 34689.0 connections\n",
      "11. Node 28304.0: 34394.0 connections\n",
      "12. Node 351694.0: 33118.0 connections\n",
      "13. Node 45434.0: 32691.0 connections\n",
      "14. Node 125196.0: 32678.0 connections\n",
      "15. Node 24712.0: 32123.0 connections\n",
      "16. Node 137953.0: 30623.0 connections\n",
      "17. Node 446734.0: 30442.0 connections\n",
      "18. Node 32426.0: 30415.0 connections\n",
      "19. Node 8970.0: 29416.0 connections\n",
      "20. Node 504981.0: 28858.0 connections\n",
      "Visualizing ego network for node 461542...\n",
      "Ego network too large (33646 nodes). Sampling 100 nodes.\n",
      "Error visualizing ego network: 461542 is not in list\n"
     ]
    }
   ],
   "source": [
    "top_nodes = find_top_nodes(\"degree_data.txt\", top_n=20, user_mapping=None)\n",
    "gc.collect()\n",
    "\n",
    "# Step 6: Visualize ego network of most connected node\n",
    "if top_nodes:\n",
    "    try:\n",
    "        visualize_ego_network(data, top_nodes[0], hops=1, max_nodes=100)\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing ego network: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeda363-c65f-4c94-845c-82f5e2509542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
